{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KurusuBlueCat/deep-lob/blob/main/colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFMM6oPutayU",
        "outputId": "1270e2cf-8176-431d-cea4-3849fa384b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARfuTY09qyZf",
        "outputId": "3a39c937-b80b-4b25-a132-87b39038f9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lteCKpUlrjbd",
        "outputId": "4bce3d48-912b-42eb-e84e-08a28d0fee9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 1 in 1.59s\n",
            "\u001b[36m> destination directory is not empty. Using --force, continuing\u001b[39m\n",
            "\u001b[36m> cloned \u001b[1mKurusuBlueCat/deep-lob\u001b[22m#\u001b[1mHEAD\u001b[22m\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "!npx degit KurusuBlueCat/deep-lob -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjt8QxHbvGKd",
        "outputId": "cac05a9e-ee2a-49db-82c6-9eecdc4d7125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.10/dist-packages (2.14.0.dev20230608)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.8 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.8.0)\n",
            "Requirement already satisfied: keras-nightly~=2.14.0.dev in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.14.0.dev2023060807)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Requirement already satisfied: tb-nightly~=2.14.0.a in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.14.0a20230608)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.3.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.14.0.dev in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.14.0.dev2023060108)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.14.0.a->tf-nightly) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_2uYvhDemQvx"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence\n",
        "from keras import layers\n",
        "from keras.callbacks import CSVLogger\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "import ast\n",
        "import os\n",
        "from typing import Union, List\n",
        "from datetime import datetime, timedelta\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dkr8phRLmQvy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "evYv4SSMmQvy"
      },
      "outputs": [],
      "source": [
        "import stride_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IQFo2FsrmQvz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vt3IpcrCtUHt"
      },
      "outputs": [],
      "source": [
        "#this path should link to a folder in your drive that contains the data required.\n",
        "#will also output run report there at output/ folder\n",
        "gdrive_data_folder = \"/content/drive/MyDrive/github_repo/deep-lob\"\n",
        "gdrive_data_folder = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XUKvF4D_mQv0"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "Chunk = namedtuple('Chunk', ['lob', 'factor', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0tEEpNePmQv0"
      },
      "outputs": [],
      "source": [
        "def get_factor_itr(factor_file_list=[f'factor_22.csv', 'factor_23.csv'], base_folder=f'{gdrive_data_folder}/factor',\n",
        "                   chunksize=500000):\n",
        "    itr_list = []\n",
        "    for f_name in factor_file_list:\n",
        "        itr_list.append(pd.read_csv(fr'{base_folder}/{f_name}', chunksize=chunksize))\n",
        "\n",
        "    for df_tuple in zip(*itr_list):\n",
        "        df_list = []\n",
        "        for df in df_tuple:\n",
        "            df['time'] = pd.to_datetime(df['time'])\n",
        "            df = df.set_index('time').sort_index()\n",
        "            # print(df.index.min(), df.index.max())\n",
        "            df = df[~df.index.duplicated()]\n",
        "            # print(df.index.min(), df.index.max())\n",
        "            df_list.append(df)\n",
        "        yield pd.concat(df_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "56Rf2YyVmQv0"
      },
      "outputs": [],
      "source": [
        "chunksize = 100000\n",
        "\n",
        "all_feature = ['volume', 'bid_1', 'bid_1', 'bid_size_1', 'ask_1',\n",
        "       'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
        "       'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
        "       'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5', 'vwap']\n",
        "\n",
        "lob_feature = ['bid_1', 'bid_size_1', 'ask_1',\n",
        "               'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
        "               'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
        "               'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5']\n",
        "\n",
        "def get_chunk_from_file_gen():\n",
        "    data_itr = pd.read_csv(f'{gdrive_data_folder}/data/data_night_shifted_au.csv.gz', chunksize=chunksize)\n",
        "    label_itr = pd.read_csv(f'{gdrive_data_folder}/label/1_min_tp4_sl4_10yuan_1delay_target.csv', chunksize=chunksize)\n",
        "    factor_itr = get_factor_itr(['factor_24.csv', 'factor_23.csv'], chunksize=chunksize)\n",
        "\n",
        "    for df, fac, label in zip(data_itr, factor_itr, label_itr):\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df = df.set_index('time').sort_index()\n",
        "\n",
        "        label['time'] = pd.to_datetime(label['time'])\n",
        "        label = label.set_index('time').sort_index()\n",
        "\n",
        "        df = df.loc[~df.index.duplicated()]\n",
        "        fac = fac.loc[~fac.index.duplicated()]\n",
        "        label = label.loc[~label.index.duplicated()]\n",
        "        # print(df.index.max(), label.index.max(), fac.index.max())\n",
        "        if (fac.index.max() != label.index.max()) or (fac.index.max() != df.index.max()):\n",
        "            raise RuntimeError(\"Index do not match!\")\n",
        "\n",
        "        yield Chunk(df, fac, label), label.index.max()\n",
        "\n",
        "def concat_chunk(*chunk_tuple):\n",
        "    arg_list = []\n",
        "    for df_tuple in zip(*chunk_tuple):\n",
        "        df = pd.concat(df_tuple).sort_index()\n",
        "        df = df.loc[~df.index.duplicated()]\n",
        "        arg_list.append(df)\n",
        "    return Chunk(*arg_list)\n",
        "\n",
        "def get_weekly_data_gen(skip_to=None):\n",
        "    print('Data loading started')\n",
        "    chunk_gen = get_chunk_from_file_gen()\n",
        "    while True:\n",
        "        chunk, latest = next(chunk_gen)\n",
        "        print(f\"                                         \", end='\\r')\n",
        "        time.sleep(0)\n",
        "        print(f\"{latest=}\", end='\\r')\n",
        "        time.sleep(0)\n",
        "        if skip_to is not None:\n",
        "            if latest < skip_to:\n",
        "                continue\n",
        "\n",
        "        if (chunk.label.index.day_of_week == 0).sum() > 0:\n",
        "            start = chunk.label[chunk.label.index.day_of_week == 0].index.date.min()\n",
        "            start: pd.Timestamp = pd.Timestamp(start)\n",
        "            end = start + pd.Timedelta(5,'d') #end on friday midnight\n",
        "            break\n",
        "\n",
        "    chunk = Chunk(*[data[data.index >= start] for data in chunk])\n",
        "    for new_chunk, latest in chunk_gen:\n",
        "        chunk = concat_chunk(chunk, new_chunk)\n",
        "        while latest > end:\n",
        "            print(f\"                                                  \", end='\\r')\n",
        "            time.sleep(0)\n",
        "            print(f\"{latest=}\", end='\\r')\n",
        "            time.sleep(0)\n",
        "            to_yield = Chunk(*(df.loc[start:end] for df in chunk))\n",
        "            chunk = Chunk(*(df.loc[end:] for df in chunk))\n",
        "            start = start + pd.Timedelta(7,'d')\n",
        "            end = end + pd.Timedelta(7,'d')\n",
        "            yield to_yield\n",
        "\n",
        "def get_chunk_list_gen(list_len=2, skip_to=None, end=None):\n",
        "    out_list = []\n",
        "    weekly_gen = get_weekly_data_gen(skip_to)\n",
        "    for week_chunk in weekly_gen:\n",
        "        if (end is not None) and (week_chunk.label.index.max() > end):\n",
        "            break \n",
        "        out_list.append(week_chunk)\n",
        "        if len(out_list) < list_len:\n",
        "            continue\n",
        "        if len(out_list) > list_len:\n",
        "            out_list = out_list[1:] #basically, pop left\n",
        "        yield out_list\n",
        "\n",
        "def split_market(chunk, avoid_market_edge = timedelta(minutes=5)):\n",
        "    chunk_arg = []\n",
        "    for df in chunk:\n",
        "        night_df = df.loc[(df.index.time >= (datetime(1970,1,1,21,0,0)+avoid_market_edge).time()) \n",
        "                        | (df.index.time <= (datetime(1970,1,1,2,30,0)-avoid_market_edge).time())] #night market\n",
        "        morning_df = df.loc[(df.index.time >= (datetime(1970,1,1,9,0,0)+avoid_market_edge).time()) \n",
        "                            & (df.index.time <= (datetime(1970,1,1,11,30,0)-avoid_market_edge).time())] #morning market\n",
        "        afternoon_df = df.loc[(df.index.time >= (datetime(1970,1,1,13,30,0)+avoid_market_edge).time())\n",
        "                            & (df.index.time <= (datetime(1970,1,1,15,0,0)-avoid_market_edge).time())] #afternoon market\n",
        "        chunk_arg.append((morning_df, afternoon_df, night_df))\n",
        "\n",
        "    return tuple(Chunk(*df_list) for df_list in zip(*chunk_arg))\n",
        "\n",
        "def get_confusion(true, pred):\n",
        "    dummy_output = pd.DataFrame(output, columns=['short', 'neutral', 'long'])\n",
        "\n",
        "    dummy_true = pd.DataFrame(true, columns=['true_short', 'true_neutral', 'true_long'])\n",
        "    result = dummy_output.join(dummy_true)\n",
        "    s_pred = result.loc[result.iloc[:,:3].max(1) == result['short']].iloc[:,3:].sum()\n",
        "    n_pred = result.loc[result.iloc[:,:3].max(1) == result['neutral']].iloc[:,3:].sum()\n",
        "    l_pred = result.loc[result.iloc[:,:3].max(1) == result['long']].iloc[:,3:].sum()\n",
        "    confusion = pd.concat([s_pred, n_pred, l_pred], axis=1)\n",
        "    confusion.columns = ['short', 'neutral', 'long']\n",
        "    return confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rt-yAH0JmQv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training_week = 8\n",
        "# skip_to = pd.Timestamp('2022-11-01 00:00:00.000') #use this to skip to the period we want\n",
        "# # skip_to = None\n",
        "# # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
        "# end = pd.Timestamp('2023-01-01 00:00:00')\n",
        "\n",
        "# chunk_list_gen = get_chunk_list_gen(training_week+1, skip_to=skip_to, end=end)\n",
        "# for i, chunk_list in enumerate(chunk_list_gen):\n",
        "#     break\n",
        "\n",
        "# train_chunk = concat_chunk(*chunk_list[:training_week])\n",
        "# walk_forward_chunk = chunk_list[-1]\n",
        "\n",
        "# morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "# morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z45dXLQVmQv2"
      },
      "outputs": [],
      "source": [
        "# lookback = 50 #this number of timestep per batch\n",
        "# batch_size = 1000 #this number of samples in a batch\n",
        "\n",
        "# chunk = morning_train_chunk\n",
        "\n",
        "# sp_list_list = []\n",
        "# for chunk in [morning_train_chunk, afternoon_train_chunk, night_train_chunk]:\n",
        "\n",
        "#     X = chunk.lob.loc[:, all_feature].join(chunk.factor)\n",
        "#     y = chunk.label\n",
        "\n",
        "#     sp_list : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(X, y.iloc[:, 0], cv=4, lookback=lookback, \n",
        "#                                             batch_size=batch_size, batch_no=None, \n",
        "#                                             shuffle=False)\n",
        "\n",
        "#     sp_list_list.append(sp_list)\n",
        "\n",
        "# sequence_pair_list_per_cv = list(zip(*sp_list_list))\n",
        "# sequence_list_per_cv = [stride_data.CombinedSequence(*[sequence_pair_list_per_cv[i][j].train_sequence \n",
        "#                         for j in range(len(sequence_pair_list_per_cv[i]))])\n",
        "#                         for i in range(len(sequence_pair_list_per_cv))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qch7UhIbxXRS"
      },
      "outputs": [],
      "source": [
        "def make_model_original(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_batchnorm(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "    inception = layers.BatchNormalization()(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    lstm = layers.BatchNormalization()(lstm)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_mz_prelu(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    cnn_act = 'linear'\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "\n",
        "    inception_act = 'linear'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.PReLU()(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.PReLU()(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.PReLU()(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.PReLU()(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.PReLU()(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.PReLU()(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "model_maker_dict = {\n",
        "    'original' : make_model_original,\n",
        "    'mz_prelu' : make_model_mz_prelu,\n",
        "    'batchnorm': make_model_batchnorm,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U8c8Zr6BmQv4"
      },
      "outputs": [],
      "source": [
        "opt_dict = {\n",
        "    'Adam': tf.keras.optimizers.Adam,\n",
        "    'Nadam': tf.keras.optimizers.Nadam,\n",
        "}\n",
        "\n",
        "# try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0b3qlLhmQv6",
        "outputId": "7e8955e7-fe6d-4a65-a324-4829d49753f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "metric_dict = {\n",
        "    # 'mse': tf.keras.metrics.MeanSquaredError(name='mean_squared_error', dtype=None),\n",
        "    # 'mae': tf.keras.metrics.MeanAbsoluteError(name='mean_absolute_error', dtype=None),\n",
        "    'coss': tf.keras.metrics.CosineSimilarity(name='cosine_similarity', dtype=None),\n",
        "    'ce': tf.keras.metrics.CategoricalCrossentropy(name='categorical_ce', dtype=None)\n",
        "}\n",
        "\n",
        "cosine = tf.keras.losses.CosineSimilarity(name='cosine_similarity')\n",
        "mse = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
        "mae = tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error')\n",
        "\n",
        "ce = tf.keras.losses.CategoricalCrossentropy(name='categorical_ce')\n",
        "fc = tf.keras.losses.CategoricalFocalCrossentropy()\n",
        "\n",
        "loss_dict = {\n",
        "    # 'mse': tf.keras.losses.MeanSquaredError(name='mean_squared_error', reduction=\"auto\"),\n",
        "    # 'mae': tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error', reduction=\"auto\"),\n",
        "    'coss': tf.keras.losses.CosineSimilarity(name='cosine_similarity', reduction=\"auto\"),\n",
        "    'ce': tf.keras.losses.CategoricalCrossentropy(name='categorical_ce', reduction='auto'),\n",
        "    'fc': tf.keras.losses.CategoricalFocalCrossentropy(),\n",
        "    # 'ce': tf.keras.losses.BinaryFocalCrossentropy(name='categorical_ce', reduction='auto'),\n",
        "    # 'coss-mse': lambda y, yhat: mse(y, yhat) + cosine(y, yhat),\n",
        "    # 'coss-mae': lambda y, yhat: mae(y, yhat) + cosine(y, yhat),\n",
        "}\n",
        "\n",
        "#           'features' :  [['High', 'Low', 'Open', 'Close', \n",
        "#                          'vix_forward_5_historical', 'vix_forward_10_historical', 'vix_forward_15_historical']],\n",
        "\n",
        "p_grid = {'batch_size' : [1024],\n",
        "          'features' :  [lob_feature],\n",
        "          'init_learning_rate' : [0.0001],\n",
        "          'lr_decay' : [1.0],\n",
        "          'loss': ['fc'],\n",
        "          'schematic' : [\n",
        "#                          'cnn_1',\n",
        "                        #  'cnn_2',\n",
        "#                          'cnn_3',\n",
        "#                          'tuesday_1',\n",
        "#                          'tuesday_2', \n",
        "                        #  'midnight_1',]}\n",
        "                         'batchnorm',]}\n",
        "#                          'midnight_6_2_rnn',\n",
        "                        #  'midnight_6_rnn',]}\n",
        "#                          'midnight_7',\n",
        "#                          'midnight_8',\n",
        "#                          'midnight_9',\n",
        "#                          'friday_1',\n",
        "#                          'friday_2',\n",
        "#                          'midnight_6_3',\n",
        "#                          'midnight_6_4',\n",
        "#                          'midnight_6_5',]}\n",
        "#                          'midnight_6_6',]}\n",
        "#                          'midnight_6_7']}\n",
        "\n",
        "p_grid = ParameterGrid(p_grid)\n",
        "len(p_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDnWegwCmQv6",
        "outputId": "823be43b-29d0-429c-eaeb-ad7a45e4374e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bid_1',\n",
              " 'bid_size_1',\n",
              " 'ask_1',\n",
              " 'ask_size_1',\n",
              " 'bid_2',\n",
              " 'bid_size_2',\n",
              " 'ask_2',\n",
              " 'ask_size_2',\n",
              " 'bid_3',\n",
              " 'bid_size_3',\n",
              " 'ask_3',\n",
              " 'ask_size_3',\n",
              " 'bid_4',\n",
              " 'bid_size_4',\n",
              " 'ask_4',\n",
              " 'ask_size_4',\n",
              " 'bid_5',\n",
              " 'bid_size_5',\n",
              " 'ask_5',\n",
              " 'ask_size_5']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lob_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "W9VX03w7mQv7"
      },
      "outputs": [],
      "source": [
        "def make_if_not_exist(folder_name, base_folder=f'{gdrive_data_folder}/output'):\n",
        "    try:\n",
        "        os.makedirs(fr'{base_folder}/{folder_name}')\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "-QtoHy-WmQv7"
      },
      "outputs": [],
      "source": [
        "from dataclasses import field, dataclass\n",
        "\n",
        "@dataclass\n",
        "class HyperParameters:\n",
        "    features: list = field(default_factory=lambda: list(all_feature))\n",
        "    lookback: int = 100\n",
        "    epochs: int = 300\n",
        "    cv: int = 5\n",
        "    batch_size: int = 1024\n",
        "    batch_no: tuple = (None, None, None) #per market period. Mornin afternoon night.\n",
        "    shuffle: bool = True\n",
        "    init_learning_rate: float = 2.5e-1\n",
        "    seed: int = 420\n",
        "    lr_decay: float = 0.95\n",
        "    trim: str = 'both'\n",
        "    decay_steps: int = 2000\n",
        "    schematic: str = 'midnight_1'\n",
        "    opt: str = 'Adam'\n",
        "    replace: bool = False\n",
        "    loss: str = 'ce'\n",
        "    training_week: int = 8\n",
        "    lob_unit : int = 32\n",
        "    inception_unit : int = 32\n",
        "    lstm_unit: int = 64\n",
        "    # skip_to = pd.Timestamp('2022-07-02 11:24:58.000') #use this to skip to the period we want\n",
        "    skip_to: pd.Timestamp = pd.Timestamp('2022-11-01 00:00:00.000')\n",
        "    # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
        "    end: pd.Timestamp = pd.Timestamp('2023-03-01 00:00:00')\n",
        "\n",
        "hp_list = [HyperParameters(**p) for p in p_grid]\n",
        "base_output_folder = f'{gdrive_data_folder}/output'\n",
        "\n",
        "run_prefix = 'test'\n",
        "\n",
        "model_dict = {}\n",
        "\n",
        "make_if_not_exist(fr\"plots/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"output/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"callback_logs/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"callback_logs/{run_prefix}/_temp\", base_output_folder)\n",
        "make_if_not_exist(fr\"run_summary/{run_prefix}\", base_output_folder)\n",
        "    \n",
        "# if run_prefix not in [d for d in os.listdir('output\\\\') if os.path.isdir('output\\\\' + d)]:\n",
        "#     os.makedirs('output\\\\' + run_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6d7T11ZmQv7",
        "outputId": "790c2fa5-5387-4ee8-d281-3efd6a83fbbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HyperParameters(features=['bid_1', 'bid_size_1', 'ask_1', 'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3', 'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4', 'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5'], lookback=100, epochs=300, cv=5, batch_size=1024, batch_no=(None, None, None), shuffle=True, init_learning_rate=0.0001, seed=420, lr_decay=1.0, trim='both', decay_steps=2000, schematic='batchnorm', opt='Adam', replace=False, loss='fc', training_week=8, lob_unit=32, inception_unit=32, lstm_unit=64, skip_to=Timestamp('2022-11-01 00:00:00'), end=Timestamp('2023-03-01 00:00:00'))\n"
          ]
        }
      ],
      "source": [
        "continue_loop = 0\n",
        "end_loop = 9999\n",
        "\n",
        "for run_count, hp in enumerate(hp_list[continue_loop:]):\n",
        "    run_count += continue_loop\n",
        "    if run_count >= end_loop:\n",
        "        break\n",
        "    print(hp)\n",
        "    if hp.trim == 'both':\n",
        "        hp.trim = (hp.lookback, hp.lookback)\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "eTMsGsZyRMtF"
      },
      "outputs": [],
      "source": [
        "def make_class(label, epsilon=1.0001):\n",
        "    long_wins = label['long_wealth'] > label['short_wealth']\n",
        "\n",
        "    class_label = pd.Series(np.nan, index=label.index)\n",
        "    class_label[label.notna().all(1)] = 0\n",
        "    class_label[long_wins & (label['long_wealth'] > epsilon)] = 1\n",
        "    class_label[~long_wins & (label['short_wealth'] > epsilon)] = -1\n",
        "    class_label = class_label.dropna().astype(int)\n",
        "    return pd.get_dummies(class_label, columns=['short', 'neutral', 'long']).astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading started\n",
            "latest=Timestamp('2023-01-05 09:32:26.500000')    \r"
          ]
        }
      ],
      "source": [
        "chunk_list_gen = get_chunk_list_gen(hp.training_week+1, skip_to=hp.skip_to, end=hp.end)\n",
        "\n",
        "conf_list = []\n",
        "\n",
        "for i, chunk_list in enumerate(chunk_list_gen):\n",
        "    break\n",
        "\n",
        "train_chunk = concat_chunk(*chunk_list[:hp.training_week])\n",
        "walk_forward_chunk = chunk_list[-1]\n",
        "\n",
        "morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proportion_by_date(chunk):\n",
        "    proportion_per_day = chunk.label.join(chunk.lob['date']).groupby('date')[['long_wealth', 'short_wealth']].apply(lambda x: (x>1).mean())\n",
        "    return proportion_per_day\n",
        "\n",
        "def index_from_date(chunk, date):\n",
        "    return chunk.lob.index[chunk.lob['date'].isin(date)]\n",
        "\n",
        "def get_sub_chunk(chunk, idx):\n",
        "    return Chunk(chunk.lob.reindex(idx), chunk.factor.reindex(idx), chunk.label.reindex(idx))\n",
        "\n",
        "def get_up_trend(chunk_list, day):\n",
        "    out_chunk_list = []\n",
        "    for chunk in chunk_list: \n",
        "        date = proportion_by_date(chunk).sort_values('long_wealth', ascending=False).index[:day]\n",
        "        date = index_from_date(chunk, date).sort_values()\n",
        "        out_chunk_list.append(get_sub_chunk(chunk, date))\n",
        "\n",
        "    return out_chunk_list\n",
        "\n",
        "def get_down_trend(chunk_list, day):\n",
        "    out_chunk_list = []\n",
        "    for chunk in chunk_list: \n",
        "        date = proportion_by_date(chunk).sort_values('short_wealth', ascending=False).index[:day]\n",
        "        date = index_from_date(chunk, date).sort_values()\n",
        "        out_chunk_list.append(get_sub_chunk(chunk, date))\n",
        "\n",
        "    return out_chunk_list\n",
        "\n",
        "def fit_to_chunk_list(chunk_list, features, transformer):\n",
        "    X_list = []\n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        y = chunk.label\n",
        "        X = X.reindex(y.index)\n",
        "        X_list.append(X)\n",
        "\n",
        "    transformer.fit(pd.concat(X_list))\n",
        "    return transformer\n",
        "\n",
        "def get_sp_generator_list(chunk_list, features, transformer, epsilon, cv, lookback, batch_size, shuffle, trim, replace):\n",
        "    sp_generator_list = []\n",
        "\n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        X = pd.DataFrame(transformer.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        sp_generator : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(\n",
        "            X, y, cv=cv, lookback=lookback, \n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle, trim=trim, replace=replace)\n",
        "\n",
        "        sp_generator_list.append(sp_generator)\n",
        "\n",
        "    return sp_generator_list\n",
        "\n",
        "def get_sequence_list(chunk_list, features, transformer, epsilon, lookback):\n",
        "    sequence_list = []\n",
        "\n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        X = pd.DataFrame(transformer.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "\n",
        "        sequence : stride_data.StrideData = stride_data.StrideData(X, y, lookback=lookback, \n",
        "                                                batch_size=y.shape[0] - lookback,\n",
        "                                                shuffle=False, replace=False)\n",
        "\n",
        "        sequence_list.append(sequence)\n",
        "\n",
        "    return sequence_list\n",
        "\n",
        "def setup_model(hp):\n",
        "    early_stopper = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=50,\n",
        "        verbose=1,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=False,\n",
        "    )\n",
        "\n",
        "    csv_logger = CSVLogger(fr'{base_output_folder}\\callback_logs\\{run_prefix}\\_temp\\{run_prefix}.csv')\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=hp.init_learning_rate,\n",
        "        decay_steps=hp.decay_steps,\n",
        "        decay_rate=hp.lr_decay)\n",
        "    try:\n",
        "        opt = opt_dict[hp.opt](learning_rate=lr_schedule)\n",
        "    except Exception:\n",
        "        print(f\"{hp.opt} does not work with scheduler\")\n",
        "        opt = opt_dict[hp.opt](learning_rate=hp.init_learning_rate)\n",
        "\n",
        "    model_maker = model_maker_dict[hp.schematic]\n",
        "\n",
        "    model = model_maker(hp.lookback, len(hp.features), hp.lob_unit, hp.inception_unit, hp.lstm_unit)\n",
        "\n",
        "    model.compile(loss=loss_dict[hp.loss], optimizer=opt, metrics=[], weighted_metrics=[])\n",
        "\n",
        "    callback_list = [early_stopper, csv_logger]\n",
        "\n",
        "    return model, callback_list\n",
        "\n",
        "uptrend_chunk_list = get_up_trend([morning_train_chunk, afternoon_train_chunk, night_train_chunk], 5)\n",
        "uptrend_chunk_list_wf = get_up_trend([morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk], 1)\n",
        "\n",
        "scaler = fit_to_chunk_list(uptrend_chunk_list, hp.features, StandardScaler())\n",
        "# sp_generator_list = get_sp_generator_list(uptrend_chunk_list, hp.features, scaler, epsilon=1.000,\n",
        "#                                           cv=hp.cv, lookback=hp.lookback, batch_size=hp.batch_size,\n",
        "#                                           shuffle=hp.shuffle, trim=hp.trim, replace=hp.replace)\n",
        "sequence_list = get_sequence_list(uptrend_chunk_list, hp.features, scaler, epsilon=1.000, lookback=hp.lookback)\n",
        "wf_sequence_list = get_sequence_list(uptrend_chunk_list_wf, hp.features, scaler, epsilon=1.000, lookback=hp.lookback)\n",
        "\n",
        "sequence = stride_data.CombinedSequence(*sequence_list, shuffle=True)\n",
        "wf_sequence = stride_data.CombinedSequence(*wf_sequence_list, shuffle=False)\n",
        "\n",
        "\n",
        "model, callback_list = setup_model(hp)\n",
        "\n",
        "sp_list = [next(sp_gen) for sp_gen in sp_generator_list]\n",
        "sq = stride_data.CombinedSequence(*[sp.train_sequence for sp in sp_list], shuffle=hp.shuffle, seed=hp.seed, replace=hp.replace)\n",
        "\n",
        "model.fit(x=sequence,\n",
        "          use_multiprocessing=False,\n",
        "          validation_data=wf_sequence,\n",
        "          epochs=hp.epochs,\n",
        "          verbose=1,\n",
        "          callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pavAQplmQv7",
        "outputId": "bab595b2-3317-413c-aee9-8ab636df6aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading started\n",
            "-1    0.151396\n",
            " 0    0.691996\n",
            " 1    0.156607\n",
            "dtype: float64\n",
            "Epoch 1/300\n",
            "686/686 [==============================] - 68s 87ms/step - loss: 0.1098 - val_loss: 0.1035\n",
            "Epoch 2/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0818 - val_loss: 0.1033\n",
            "Epoch 3/300\n",
            "686/686 [==============================] - 52s 76ms/step - loss: 0.0796 - val_loss: 0.1053\n",
            "Epoch 4/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0778 - val_loss: 0.1079\n",
            "Epoch 5/300\n",
            "686/686 [==============================] - 52s 76ms/step - loss: 0.0747 - val_loss: 0.1140\n",
            "Epoch 6/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0692 - val_loss: 0.1263\n",
            "Epoch 7/300\n",
            "686/686 [==============================] - 71s 104ms/step - loss: 0.0616 - val_loss: 0.1357\n",
            "Epoch 8/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0545 - val_loss: 0.1484\n",
            "Epoch 9/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0487 - val_loss: 0.1635\n",
            "Epoch 10/300\n",
            "686/686 [==============================] - 72s 104ms/step - loss: 0.0440 - val_loss: 0.1794\n",
            "Epoch 11/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0404 - val_loss: 0.1896\n",
            "Epoch 12/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0372 - val_loss: 0.2007\n",
            "Epoch 13/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0344 - val_loss: 0.2133\n",
            "Epoch 14/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0318 - val_loss: 0.2270\n",
            "Epoch 15/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0296 - val_loss: 0.2339\n",
            "Epoch 16/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0274 - val_loss: 0.2416\n",
            "Epoch 17/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0258 - val_loss: 0.2557\n",
            "Epoch 18/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0238 - val_loss: 0.2665\n",
            "Epoch 19/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0223 - val_loss: 0.2737\n",
            "Epoch 20/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0208 - val_loss: 0.2865\n",
            "Epoch 21/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0191 - val_loss: 0.2940\n",
            "Epoch 22/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0177 - val_loss: 0.3011\n",
            "Epoch 23/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0162 - val_loss: 0.3084\n",
            "Epoch 24/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0151 - val_loss: 0.3212\n",
            "Epoch 25/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0139 - val_loss: 0.3327\n",
            "Epoch 26/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0130 - val_loss: 0.3359\n",
            "Epoch 27/300\n",
            "686/686 [==============================] - 52s 76ms/step - loss: 0.0118 - val_loss: 0.3549\n",
            "Epoch 28/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0107 - val_loss: 0.3600\n",
            "Epoch 29/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0100 - val_loss: 0.3822\n",
            "Epoch 30/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0091 - val_loss: 0.3794\n",
            "Epoch 31/300\n",
            "686/686 [==============================] - 52s 76ms/step - loss: 0.0082 - val_loss: 0.3968\n",
            "Epoch 32/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0075 - val_loss: 0.4036\n",
            "Epoch 33/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0069 - val_loss: 0.4197\n",
            "Epoch 34/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0062 - val_loss: 0.4285\n",
            "Epoch 35/300\n",
            "686/686 [==============================] - 54s 79ms/step - loss: 0.0060 - val_loss: 0.4419\n",
            "Epoch 36/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0054 - val_loss: 0.4451\n",
            "Epoch 37/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0047 - val_loss: 0.4603\n",
            "Epoch 38/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0043 - val_loss: 0.4656\n",
            "Epoch 39/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0037 - val_loss: 0.4799\n",
            "Epoch 40/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0034 - val_loss: 0.4949\n",
            "Epoch 41/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0032 - val_loss: 0.5172\n",
            "Epoch 42/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0027 - val_loss: 0.5091\n",
            "Epoch 43/300\n",
            "686/686 [==============================] - 54s 79ms/step - loss: 0.0026 - val_loss: 0.5244\n",
            "Epoch 44/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0025 - val_loss: 0.5308\n",
            "Epoch 45/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0021 - val_loss: 0.5479\n",
            "Epoch 46/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0020 - val_loss: 0.5508\n",
            "Epoch 47/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0015 - val_loss: 0.5577\n",
            "Epoch 48/300\n",
            "686/686 [==============================] - 54s 78ms/step - loss: 0.0017 - val_loss: 0.5677\n",
            "Epoch 49/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0017 - val_loss: 0.5751\n",
            "Epoch 50/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0016 - val_loss: 0.5847\n",
            "Epoch 51/300\n",
            "686/686 [==============================] - 53s 78ms/step - loss: 0.0015 - val_loss: 0.5788\n",
            "Epoch 52/300\n",
            "686/686 [==============================] - 53s 77ms/step - loss: 0.0010 - val_loss: 0.5822\n",
            "Epoch 52: early stopping\n",
            "1093/1093 [==============================] - 6s 5ms/step\n",
            "596/596 [==============================] - 2s 4ms/step\n",
            "2123/2123 [==============================] - 9s 4ms/step\n",
            "-1    0.163551\n",
            " 0    0.668568\n",
            " 1    0.167881\n",
            "dtype: float64\n",
            "Epoch 1/300\n",
            "725/725 [==============================] - ETA: 0s - loss: 0.1116"
          ]
        }
      ],
      "source": [
        "chunk_list_gen = get_chunk_list_gen(hp.training_week+1, skip_to=hp.skip_to, end=hp.end)\n",
        "\n",
        "conf_list = []\n",
        "\n",
        "for i, chunk_list in enumerate(chunk_list_gen):\n",
        "\n",
        "    train_chunk = concat_chunk(*chunk_list[:hp.training_week])\n",
        "    walk_forward_chunk = chunk_list[-1]\n",
        "\n",
        "    morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "    morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)\n",
        "\n",
        "    print(make_class(night_train_chunk.label, 1.0000).mean())\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    epsilon=1.0000\n",
        "\n",
        "    for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        X_list.append(X)\n",
        "        y_list.append(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(pd.concat(X_list))\n",
        "\n",
        "    sp_generator_list = []\n",
        "\n",
        "    for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
        "\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        sp_generator : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(\n",
        "            X, y, cv=hp.cv, lookback=hp.lookback, \n",
        "            batch_size=hp.batch_size, batch_no=hp.batch_no[i], \n",
        "            shuffle=hp.shuffle, trim=hp.trim, replace=hp.replace)\n",
        "\n",
        "        sp_generator_list.append(sp_generator)\n",
        "\n",
        "    wf_sequence_list = []\n",
        "\n",
        "    for i, chunk in enumerate([morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk]):\n",
        "\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        wf_sequence : stride_data.StrideData = stride_data.StrideData(X, y, lookback=hp.lookback, \n",
        "                                                batch_size=y.shape[0] - hp.lookback, batch_no=None, \n",
        "                                                shuffle=False, replace=False)\n",
        "\n",
        "        wf_sequence_list.append(wf_sequence)\n",
        "\n",
        "    early_stopper = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=50,\n",
        "        verbose=1,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=False,\n",
        "    )\n",
        "\n",
        "    # for cv_count in range(hp.cv): #just have train-validation\n",
        "    sp_list = [next(sp_gen) for sp_gen in sp_generator_list]\n",
        "    sq = stride_data.CombinedSequence(*[sp.train_sequence for sp in sp_list], shuffle=hp.shuffle, seed=hp.seed, replace=hp.replace)\n",
        "    test_tuple = np.vstack([sp.test_tuple[0] for sp in sp_list]), np.vstack([sp.test_tuple[1] for sp in sp_list])\n",
        "\n",
        "    csv_logger = CSVLogger(fr'{base_output_folder}\\callback_logs\\{run_prefix}\\_temp\\{run_prefix}.csv')\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=hp.init_learning_rate,\n",
        "        decay_steps=hp.decay_steps,\n",
        "        decay_rate=hp.lr_decay)\n",
        "    try:\n",
        "        opt = opt_dict[hp.opt](learning_rate=lr_schedule)\n",
        "    except Exception:\n",
        "        print(f\"{hp.opt} does not work with scheduler\")\n",
        "        opt = opt_dict[hp.opt](learning_rate=hp.init_learning_rate)\n",
        "\n",
        "    # model = tf.keras.Sequential()\n",
        "    # for l, p in schematic_dict[hp.schematic]:\n",
        "    #     model.add(l(**p))\n",
        "        \n",
        "    # model.add(layers.Dense(3, activation='softmax')) # add output node\n",
        "\n",
        "    model_maker = model_maker_dict[hp.schematic]\n",
        "\n",
        "    model = model_maker(hp.lookback, len(hp.features), hp.lob_unit, hp.inception_unit, hp.lstm_unit)\n",
        "\n",
        "    model.compile(loss=loss_dict[hp.loss], optimizer=opt, metrics=[], weighted_metrics=[])\n",
        "    # with tf.device('/cpu:0'):\n",
        "    model.fit(x=sq,\n",
        "            use_multiprocessing=False,\n",
        "            validation_data=test_tuple,\n",
        "            epochs=hp.epochs,\n",
        "            verbose=1,\n",
        "            callbacks=[csv_logger, early_stopper])\n",
        "\n",
        "    confusion = pd.DataFrame(0, \n",
        "                            index=['true_short', 'true_neutral', 'true_long'],\n",
        "                            columns=['short', 'neutral', 'long'])\n",
        "    for wf in wf_sequence_list:\n",
        "        output = model.predict(wf[0][0])\n",
        "        confusion += get_confusion(wf[0][1], output)\n",
        "    conf_list.append(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt0VhHFtmQv9"
      },
      "outputs": [],
      "source": [
        "print(conf_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erkTXS3zLPYf"
      },
      "outputs": [],
      "source": [
        "wf_sequence_list[0].get_target_indices(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67w68xCGTH7x"
      },
      "outputs": [],
      "source": [
        "confusion = pd.DataFrame(0, \n",
        "                        index=['true_short', 'true_neutral', 'true_long'],\n",
        "                        columns=['short', 'neutral', 'long'])\n",
        "for wf in wf_sequence_list:\n",
        "    output = model.predict(wf[0][0])\n",
        "    confusion += get_confusion(wf[0][1], output)\n",
        "\n",
        "print(confusion)\n",
        "print(confusion/confusion.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XjTpB0DcnB6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
