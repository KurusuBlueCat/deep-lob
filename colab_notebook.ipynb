{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KurusuBlueCat/deep-lob/blob/main/colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFMM6oPutayU",
        "outputId": "7c534088-5f5d-4136-84dd-65de26561562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARfuTY09qyZf",
        "outputId": "410dc0cd-69d8-45eb-ab28-8ed35b83e3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lteCKpUlrjbd",
        "outputId": "12abb770-dca0-4d8e-f76c-9ee068c0c034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 1 in 2.133s\n",
            "\u001b[36m> destination directory is not empty. Using --force, continuing\u001b[39m\n",
            "\u001b[36m> cloned \u001b[1mKurusuBlueCat/deep-lob\u001b[22m#\u001b[1mHEAD\u001b[22m\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "!npx degit KurusuBlueCat/deep-lob -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjt8QxHbvGKd",
        "outputId": "5812fb23-36cb-4b27-f516-c4ed97b7c289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.14.0.dev20230609-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Collecting flatbuffers>=23.5.8 (from tf-nightly)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.8.0)\n",
            "Collecting keras-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-2.14.0.dev2023060907-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Collecting tb-nightly~=2.14.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.14.0a20230609-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.3.0)\n",
            "Collecting tf-estimator-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading tf_estimator_nightly-2.14.0.dev2023060108-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.0/441.0 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.14.0.a->tf-nightly) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tf-estimator-nightly, keras-nightly, tb-nightly, tf-nightly\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "Successfully installed flatbuffers-23.5.26 keras-nightly-2.14.0.dev2023060907 tb-nightly-2.14.0a20230609 tf-estimator-nightly-2.14.0.dev2023060108 tf-nightly-2.14.0.dev20230609\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_2uYvhDemQvx"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence\n",
        "from keras import layers\n",
        "from keras.callbacks import CSVLogger\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "import ast\n",
        "import os\n",
        "from typing import Union, List\n",
        "from datetime import datetime, timedelta\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dkr8phRLmQvy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "evYv4SSMmQvy"
      },
      "outputs": [],
      "source": [
        "import stride_data\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IQFo2FsrmQvz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vt3IpcrCtUHt"
      },
      "outputs": [],
      "source": [
        "#this path should link to a folder in your drive that contains the data required.\n",
        "#will also output run report there at output/ folder\n",
        "gdrive_data_folder = \"/content/drive/MyDrive/github_repo/deep-lob\"\n",
        "# gdrive_data_folder = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XUKvF4D_mQv0"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "Chunk = namedtuple('Chunk', ['lob', 'factor', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0tEEpNePmQv0"
      },
      "outputs": [],
      "source": [
        "def get_factor_itr(factor_file_list=[f'factor_22.csv', 'factor_23.csv'], base_folder=f'{gdrive_data_folder}/factor',\n",
        "                   chunksize=500000):\n",
        "    itr_list = []\n",
        "    for f_name in factor_file_list:\n",
        "        itr_list.append(pd.read_csv(fr'{base_folder}/{f_name}', chunksize=chunksize))\n",
        "\n",
        "    for df_tuple in zip(*itr_list):\n",
        "        df_list = []\n",
        "        for df in df_tuple:\n",
        "            df['time'] = pd.to_datetime(df['time'])\n",
        "            df = df.set_index('time').sort_index()\n",
        "            # print(df.index.min(), df.index.max())\n",
        "            df = df[~df.index.duplicated()]\n",
        "            # print(df.index.min(), df.index.max())\n",
        "            df_list.append(df)\n",
        "        yield pd.concat(df_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "56Rf2YyVmQv0"
      },
      "outputs": [],
      "source": [
        "chunksize = 100000\n",
        "\n",
        "all_feature = ['volume', 'bid_1', 'bid_1', 'bid_size_1', 'ask_1',\n",
        "       'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
        "       'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
        "       'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5', 'vwap']\n",
        "\n",
        "lob_feature = ['bid_1', 'bid_size_1', 'ask_1',\n",
        "               'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
        "               'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
        "               'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5']\n",
        "\n",
        "def get_chunk_from_file_gen():\n",
        "    data_itr = pd.read_csv(f'{gdrive_data_folder}/data/data_night_shifted_au.csv.gz', chunksize=chunksize)\n",
        "    label_itr = pd.read_csv(f'{gdrive_data_folder}/label/1_min_tp4_sl2_10yuan_1delay_target.csv', chunksize=chunksize)\n",
        "    factor_itr = get_factor_itr(['factor_24.csv', 'factor_23.csv'], chunksize=chunksize)\n",
        "\n",
        "    for df, fac, label in zip(data_itr, factor_itr, label_itr):\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df = df.set_index('time').sort_index()\n",
        "\n",
        "        label['time'] = pd.to_datetime(label['time'])\n",
        "        label = label.set_index('time').sort_index()\n",
        "\n",
        "        df = df.loc[~df.index.duplicated()]\n",
        "        fac = fac.loc[~fac.index.duplicated()]\n",
        "        label = label.loc[~label.index.duplicated()]\n",
        "        # print(df.index.max(), label.index.max(), fac.index.max())\n",
        "        if (fac.index.max() != label.index.max()) or (fac.index.max() != df.index.max()):\n",
        "            raise RuntimeError(\"Index do not match!\")\n",
        "\n",
        "        yield Chunk(df, fac, label), label.index.max()\n",
        "\n",
        "def concat_chunk(*chunk_tuple):\n",
        "    arg_list = []\n",
        "    for df_tuple in zip(*chunk_tuple):\n",
        "        df = pd.concat(df_tuple).sort_index()\n",
        "        df = df.loc[~df.index.duplicated()]\n",
        "        arg_list.append(df)\n",
        "    return Chunk(*arg_list)\n",
        "\n",
        "def get_weekly_data_gen(skip_to=None):\n",
        "    print('Data loading started')\n",
        "    chunk_gen = get_chunk_from_file_gen()\n",
        "    while True:\n",
        "        chunk, latest = next(chunk_gen)\n",
        "        print(f\"                                         \", end='\\r')\n",
        "        time.sleep(0)\n",
        "        print(f\"{latest=}\", end='\\r')\n",
        "        time.sleep(0)\n",
        "        if skip_to is not None:\n",
        "            if latest < skip_to:\n",
        "                continue\n",
        "\n",
        "        if (chunk.label.index.day_of_week == 0).sum() > 0:\n",
        "            start = chunk.label[chunk.label.index.day_of_week == 0].index.date.min()\n",
        "            start: pd.Timestamp = pd.Timestamp(start)\n",
        "            end = start + pd.Timedelta(5,'d') #end on friday midnight\n",
        "            break\n",
        "\n",
        "    chunk = Chunk(*[data[data.index >= start] for data in chunk])\n",
        "    for new_chunk, latest in chunk_gen:\n",
        "        chunk = concat_chunk(chunk, new_chunk)\n",
        "        while latest > end:\n",
        "            print(f\"                                                  \", end='\\r')\n",
        "            time.sleep(0)\n",
        "            print(f\"{latest=}\", end='\\r')\n",
        "            time.sleep(0)\n",
        "            to_yield = Chunk(*(df.loc[start:end] for df in chunk))\n",
        "            chunk = Chunk(*(df.loc[end:] for df in chunk))\n",
        "            start = start + pd.Timedelta(7,'d')\n",
        "            end = end + pd.Timedelta(7,'d')\n",
        "            yield to_yield\n",
        "\n",
        "def get_chunk_list_gen(list_len=2, skip_to=None, end=None):\n",
        "    out_list = []\n",
        "    weekly_gen = get_weekly_data_gen(skip_to)\n",
        "    for week_chunk in weekly_gen:\n",
        "        if (end is not None) and (week_chunk.label.index.max() > end):\n",
        "            break \n",
        "        out_list.append(week_chunk)\n",
        "        if len(out_list) < list_len:\n",
        "            continue\n",
        "        if len(out_list) > list_len:\n",
        "            out_list = out_list[1:] #basically, pop left\n",
        "        yield out_list\n",
        "\n",
        "def split_market(chunk, avoid_market_edge = timedelta(minutes=5)):\n",
        "    chunk_arg = []\n",
        "    for df in chunk:\n",
        "        night_df = df.loc[(df.index.time >= (datetime(1970,1,1,21,0,0)+avoid_market_edge).time()) \n",
        "                        | (df.index.time <= (datetime(1970,1,1,2,30,0)-avoid_market_edge).time())] #night market\n",
        "        morning_df = df.loc[(df.index.time >= (datetime(1970,1,1,9,0,0)+avoid_market_edge).time()) \n",
        "                            & (df.index.time <= (datetime(1970,1,1,11,30,0)-avoid_market_edge).time())] #morning market\n",
        "        afternoon_df = df.loc[(df.index.time >= (datetime(1970,1,1,13,30,0)+avoid_market_edge).time())\n",
        "                            & (df.index.time <= (datetime(1970,1,1,15,0,0)-avoid_market_edge).time())] #afternoon market\n",
        "        chunk_arg.append((morning_df, afternoon_df, night_df))\n",
        "\n",
        "    return tuple(Chunk(*df_list) for df_list in zip(*chunk_arg))\n",
        "\n",
        "def get_confusion(true, pred):\n",
        "    pred = pd.DataFrame(pred, columns=['short', 'neutral', 'long'])\n",
        "\n",
        "    true = pd.DataFrame(true, columns=['true_short', 'true_neutral', 'true_long'])\n",
        "    result = pred.join(true)\n",
        "    s_pred = result.loc[result.iloc[:,:3].max(1) == result['short']].iloc[:,3:].sum()\n",
        "    n_pred = result.loc[result.iloc[:,:3].max(1) == result['neutral']].iloc[:,3:].sum()\n",
        "    l_pred = result.loc[result.iloc[:,:3].max(1) == result['long']].iloc[:,3:].sum()\n",
        "    confusion = pd.concat([s_pred, n_pred, l_pred], axis=1)\n",
        "    confusion.columns = ['short', 'neutral', 'long']\n",
        "    return confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rt-yAH0JmQv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training_week = 8\n",
        "# skip_to = pd.Timestamp('2022-11-01 00:00:00.000') #use this to skip to the period we want\n",
        "# # skip_to = None\n",
        "# # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
        "# end = pd.Timestamp('2023-01-01 00:00:00')\n",
        "\n",
        "# chunk_list_gen = get_chunk_list_gen(training_week+1, skip_to=skip_to, end=end)\n",
        "# for i, chunk_list in enumerate(chunk_list_gen):\n",
        "#     break\n",
        "\n",
        "# train_chunk = concat_chunk(*chunk_list[:training_week])\n",
        "# walk_forward_chunk = chunk_list[-1]\n",
        "\n",
        "# morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "# morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z45dXLQVmQv2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# lookback = 50 #this number of timestep per batch\n",
        "# batch_size = 1000 #this number of samples in a batch\n",
        "\n",
        "# chunk = morning_train_chunk\n",
        "\n",
        "# sp_list_list = []\n",
        "# for chunk in [morning_train_chunk, afternoon_train_chunk, night_train_chunk]:\n",
        "\n",
        "#     X = chunk.lob.loc[:, all_feature].join(chunk.factor)\n",
        "#     y = chunk.label\n",
        "\n",
        "#     sp_list : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(X, y.iloc[:, 0], cv=4, lookback=lookback, \n",
        "#                                             batch_size=batch_size, batch_no=None, \n",
        "#                                             shuffle=False)\n",
        "\n",
        "#     sp_list_list.append(sp_list)\n",
        "\n",
        "# sequence_pair_list_per_cv = list(zip(*sp_list_list))\n",
        "# sequence_list_per_cv = [stride_data.CombinedSequence(*[sequence_pair_list_per_cv[i][j].train_sequence \n",
        "#                         for j in range(len(sequence_pair_list_per_cv[i]))])\n",
        "#                         for i in range(len(sequence_pair_list_per_cv))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qch7UhIbxXRS"
      },
      "outputs": [],
      "source": [
        "def make_model_original(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_adhoc(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "    inception = layers.Dropout(0.2)(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit, kernel_regularizer='l2', recurrent_regularizer='l2')(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_dropout_madness(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.Dropout(0.2)(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "    inception = layers.Dropout(0.2)(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit, kernel_regularizer='l2', recurrent_regularizer='l2')(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_batchnorm(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5))(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same')(lob_layer)\n",
        "    lob_layer = layers.LeakyReLU(alpha=0.01)(lob_layer)\n",
        "    lob_layer = layers.BatchNormalization()(lob_layer)\n",
        "    inception_act = 'leaky_relu'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.LeakyReLU(alpha=0.01)(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.LeakyReLU(alpha=0.01)(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.LeakyReLU(alpha=0.01)(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "    inception = layers.BatchNormalization()(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    lstm = layers.BatchNormalization()(lstm)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_model_mz_prelu(lookback, feature_no, lob_unit, inception_unit, lstm_unit, *args, **kwargs):\n",
        "\n",
        "    input_layer = layers.Input(shape=(lookback, feature_no))\n",
        "    lob_layer = layers.Reshape(target_shape=(lookback,feature_no,1))(input_layer)\n",
        "\n",
        "    cnn_act = 'linear'\n",
        "\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,2), strides=(1,2), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(1,5), activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "    lob_layer = layers.Conv2D(filters=lob_unit, kernel_size=(5,1), padding='same', activation=cnn_act)(lob_layer)\n",
        "    lob_layer = layers.PReLU()(lob_layer)\n",
        "\n",
        "    inception_act = 'linear'\n",
        "\n",
        "    inception_1 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_1 = layers.PReLU()(inception_1)\n",
        "    inception_1 = layers.Conv2D(inception_unit, (3,1), padding='same', activation=inception_act)(inception_1)\n",
        "    inception_1 = layers.PReLU()(inception_1)\n",
        "\n",
        "    inception_2 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(lob_layer)\n",
        "    inception_2 = layers.PReLU()(inception_2)\n",
        "    inception_2 = layers.Conv2D(inception_unit, (5,1), padding='same', activation=inception_act)(inception_2)\n",
        "    inception_2 = layers.PReLU()(inception_2)\n",
        "\n",
        "    inception_3 = layers.MaxPooling2D((3,1), strides=(1,1), padding='same')(lob_layer)\n",
        "    inception_3 = layers.PReLU()(inception_3)\n",
        "    inception_3 = layers.Conv2D(inception_unit, (1,1), padding='same', activation=inception_act)(inception_3)\n",
        "    inception_3 = layers.PReLU()(inception_3)\n",
        "\n",
        "    inception = tf.keras.layers.concatenate([inception_1, inception_2, inception_3], axis=3)\n",
        "    inception = layers.Reshape((lookback, inception_unit*3))(inception)\n",
        "\n",
        "    lstm = layers.LSTM(lstm_unit)(inception)\n",
        "    output = layers.Dense(3, activation='softmax')(lstm)\n",
        "\n",
        "    model = keras.Model(input_layer, output)\n",
        "\n",
        "    return model\n",
        "\n",
        "model_maker_dict = {\n",
        "    'original' : make_model_original,\n",
        "    'adhoc' : make_model_adhoc,\n",
        "    'mz_prelu' : make_model_mz_prelu,\n",
        "    'dropout' : make_model_dropout_madness,\n",
        "    'batchnorm': make_model_batchnorm,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U8c8Zr6BmQv4"
      },
      "outputs": [],
      "source": [
        "opt_dict = {\n",
        "    'Adam': tf.keras.optimizers.Adam,\n",
        "    'Nadam': tf.keras.optimizers.Nadam,\n",
        "}\n",
        "\n",
        "# try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0b3qlLhmQv6",
        "outputId": "41a791b6-eac1-4c8f-b224-aa77a22f861d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "metric_dict = {\n",
        "    # 'mse': tf.keras.metrics.MeanSquaredError(name='mean_squared_error', dtype=None),\n",
        "    # 'mae': tf.keras.metrics.MeanAbsoluteError(name='mean_absolute_error', dtype=None),\n",
        "    'coss': tf.keras.metrics.CosineSimilarity(name='cosine_similarity', dtype=None),\n",
        "    'ce': tf.keras.metrics.CategoricalCrossentropy(name='categorical_ce', dtype=None)\n",
        "}\n",
        "\n",
        "cosine = tf.keras.losses.CosineSimilarity(name='cosine_similarity')\n",
        "mse = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
        "mae = tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error')\n",
        "\n",
        "ce = tf.keras.losses.CategoricalCrossentropy(name='categorical_ce')\n",
        "fc = tf.keras.losses.CategoricalFocalCrossentropy()\n",
        "\n",
        "loss_dict = {\n",
        "    # 'mse': tf.keras.losses.MeanSquaredError(name='mean_squared_error', reduction=\"auto\"),\n",
        "    # 'mae': tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error', reduction=\"auto\"),\n",
        "    'coss': tf.keras.losses.CosineSimilarity(name='cosine_similarity', reduction=\"auto\"),\n",
        "    'ce': tf.keras.losses.CategoricalCrossentropy(name='categorical_ce', reduction='auto'),\n",
        "    'fc': tf.keras.losses.CategoricalFocalCrossentropy(),\n",
        "    # 'ce': tf.keras.losses.BinaryFocalCrossentropy(name='categorical_ce', reduction='auto'),\n",
        "    # 'coss-mse': lambda y, yhat: mse(y, yhat) + cosine(y, yhat),\n",
        "    # 'coss-mae': lambda y, yhat: mae(y, yhat) + cosine(y, yhat),\n",
        "}\n",
        "\n",
        "#           'features' :  [['High', 'Low', 'Open', 'Close', \n",
        "#                          'vix_forward_5_historical', 'vix_forward_10_historical', 'vix_forward_15_historical']],\n",
        "\n",
        "# p_grid = {'batch_size' : [128],\n",
        "#           'features' :  [lob_feature],\n",
        "#           'init_learning_rate' : [0.0005, 0.001],\n",
        "#           'lr_decay' : [1.0],\n",
        "#           'loss': ['fc'],\n",
        "#           'lob_unit': [16],\n",
        "#           'inception_unit' : [16],\n",
        "#           'lstm_unit': [32],\n",
        "#           'trend_proportion': [(1/6, 1/6, 1/6),(1/4, 1/4, 1/4)],\n",
        "#           'schematic' : ['adhoc',]}\n",
        "\n",
        "# p_grid = {}\n",
        "\n",
        "p_grid2 = {'batch_size' : [256, 128],\n",
        "          'features' :  [lob_feature],\n",
        "          'init_learning_rate' : [0.0001],\n",
        "          'lr_decay' : [1.0],\n",
        "          'loss': ['fc'],\n",
        "          'lob_unit': [24],\n",
        "          'inception_unit' : [24],\n",
        "          'lstm_unit': [48],\n",
        "          'trend_proportion': [(1/6, 1/6, 1/6),(1/4, 1/4, 1/4)],\n",
        "          'schematic' : ['adhoc',]}\n",
        "\n",
        "p_grid = list(ParameterGrid(p_grid2))\n",
        "len(p_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDnWegwCmQv6",
        "outputId": "b9e51e94-0e3b-40a4-81b2-b515cafd9311"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bid_1',\n",
              " 'bid_size_1',\n",
              " 'ask_1',\n",
              " 'ask_size_1',\n",
              " 'bid_2',\n",
              " 'bid_size_2',\n",
              " 'ask_2',\n",
              " 'ask_size_2',\n",
              " 'bid_3',\n",
              " 'bid_size_3',\n",
              " 'ask_3',\n",
              " 'ask_size_3',\n",
              " 'bid_4',\n",
              " 'bid_size_4',\n",
              " 'ask_4',\n",
              " 'ask_size_4',\n",
              " 'bid_5',\n",
              " 'bid_size_5',\n",
              " 'ask_5',\n",
              " 'ask_size_5']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "lob_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "W9VX03w7mQv7"
      },
      "outputs": [],
      "source": [
        "def make_if_not_exist(folder_name, base_folder=f'{gdrive_data_folder}/output'):\n",
        "    try:\n",
        "        os.makedirs(fr'{base_folder}/{folder_name}')\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-QtoHy-WmQv7"
      },
      "outputs": [],
      "source": [
        "from dataclasses import field, dataclass\n",
        "\n",
        "@dataclass\n",
        "class HyperParameters:\n",
        "    features: list = field(default_factory=lambda: list(all_feature))\n",
        "    lookback: int = 50\n",
        "    epochs: int = 300\n",
        "    cv: int = 5\n",
        "    batch_size: int = 1024\n",
        "    batch_no: tuple = (None, None, None) #per market period. Mornin afternoon night.\n",
        "    shuffle: bool = True\n",
        "    init_learning_rate: float = 2.5e-1\n",
        "    seed: int = 420\n",
        "    lr_decay: float = 0.95\n",
        "    trim: str = 'both'\n",
        "    decay_steps: int = 2000\n",
        "    schematic: str = 'midnight_1'\n",
        "    opt: str = 'Adam'\n",
        "    replace: bool = False\n",
        "    loss: str = 'ce'\n",
        "    training_week: int = 8\n",
        "    lob_unit : int = 16\n",
        "    inception_unit : int = 16\n",
        "    lstm_unit: int = 32\n",
        "    trend_proportion: tuple = (0, 1/8, 1/8)\n",
        "    min_proportion: bool = False\n",
        "    randomize_neutral: bool = True\n",
        "    # skip_to = pd.Timestamp('2022-07-02 11:24:58.000') #use this to skip to the period we want\n",
        "    skip_to: str = '2022-11-01 00:00:00.000'\n",
        "    # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
        "    end: str = '2023-03-01 00:00:00'\n",
        "\n",
        "hp_list = [HyperParameters(**p) for p in p_grid]\n",
        "base_output_folder = f'{gdrive_data_folder}/output'\n",
        "\n",
        "run_prefix = 'subset_test'\n",
        "\n",
        "model_dict = {}\n",
        "\n",
        "make_if_not_exist(fr\"plots/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"output/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"callback_logs/{run_prefix}\", base_output_folder)\n",
        "make_if_not_exist(fr\"callback_logs/{run_prefix}/_temp\", base_output_folder)\n",
        "make_if_not_exist(fr\"run_summary/{run_prefix}\", base_output_folder)\n",
        "    \n",
        "# if run_prefix not in [d for d in os.listdir('output\\\\') if os.path.isdir('output\\\\' + d)]:\n",
        "#     os.makedirs('output\\\\' + run_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "k7UgpGPK1K4u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "def get_count(folder_path):\n",
        "    try:\n",
        "        with open(f'{folder_path}/counter.txt', 'r') as f:\n",
        "            no = f.read()\n",
        "            try:\n",
        "                no = int(no)\n",
        "            except ValueError:\n",
        "                no = -1\n",
        "    except FileNotFoundError:\n",
        "        no = -1\n",
        "\n",
        "    with open(f'{folder_path}/counter.txt', 'w+') as f:\n",
        "        f.write(str(no+1))\n",
        "\n",
        "    return no+1\n",
        "\n",
        "def make_class(label, epsilon=1.0001):\n",
        "    long_wins = label['long_wealth'] > label['short_wealth']\n",
        "\n",
        "    class_label = pd.Series(np.nan, index=label.index)\n",
        "    class_label[label.notna().all(1)] = 0\n",
        "    class_label[long_wins & (label['long_wealth'] > epsilon)] = 1\n",
        "    class_label[~long_wins & (label['short_wealth'] > epsilon)] = -1\n",
        "    class_label = class_label.dropna().astype(int)\n",
        "    output = pd.get_dummies(class_label, columns=['short', 'neutral', 'long']).astype(int)\n",
        "    output['short'] = 0\n",
        "    return output\n",
        "\n",
        "def proportion_by_date(chunk):\n",
        "    proportion_per_day = chunk.label.join(chunk.lob['date']).groupby('date')[['long_wealth', 'short_wealth']].apply(lambda x: (x>1).mean())\n",
        "    return proportion_per_day\n",
        "\n",
        "def index_from_date(chunk, date):\n",
        "    return chunk.lob.index[chunk.lob['date'].isin(date)]\n",
        "\n",
        "def get_sub_chunk(chunk, idx):\n",
        "    return Chunk(chunk.lob.reindex(idx), chunk.factor.reindex(idx), chunk.label.reindex(idx))\n",
        "\n",
        "def get_up_trend(chunk_list, day):\n",
        "    out_chunk_list = []\n",
        "    for chunk in chunk_list: \n",
        "        date = proportion_by_date(chunk).sort_values('long_wealth', ascending=False).index[:day]\n",
        "        date = index_from_date(chunk, date).sort_values()\n",
        "        out_chunk_list.append(get_sub_chunk(chunk, date))\n",
        "\n",
        "    return out_chunk_list\n",
        "\n",
        "def get_down_trend(chunk_list, day):\n",
        "    out_chunk_list = []\n",
        "    for chunk in chunk_list: \n",
        "        date = proportion_by_date(chunk).sort_values('short_wealth', ascending=False).index[:day]\n",
        "        date = index_from_date(chunk, date).sort_values()\n",
        "        out_chunk_list.append(get_sub_chunk(chunk, date))\n",
        "\n",
        "    return out_chunk_list\n",
        "\n",
        "def fit_to_chunk_list(chunk_list, features, transformer):\n",
        "    X_list = []\n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        y = chunk.label\n",
        "        X = X.reindex(y.index)\n",
        "        X_list.append(X)\n",
        "\n",
        "    transformer.fit(pd.concat(X_list))\n",
        "    return transformer\n",
        "\n",
        "def get_sp_generator_list(chunk_list, features, transformer, epsilon, cv, lookback, batch_size, shuffle, trim, replace):\n",
        "    sp_generator_list = []\n",
        "\n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        X = pd.DataFrame(transformer.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        sp_generator : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(\n",
        "            X, y, cv=cv, lookback=lookback, \n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle, trim=trim, replace=replace)\n",
        "\n",
        "        sp_generator_list.append(sp_generator)\n",
        "\n",
        "    return sp_generator_list\n",
        "\n",
        "def get_sequence_list(chunk_list, features, transformer, epsilon, lookback, shuffle=False, replace=False, batch_size=128,\n",
        "                      seed=420):\n",
        "    sequence_list = []\n",
        "    \n",
        "    for chunk in chunk_list:\n",
        "        X = chunk.lob.loc[:, features]\n",
        "        X = pd.DataFrame(transformer.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "\n",
        "        bs = min(batch_size, y.shape[0] - lookback)\n",
        "        sequence : stride_data.StrideData = stride_data.StrideData(X, y, lookback=lookback, \n",
        "                                                batch_size=bs,\n",
        "                                                shuffle=shuffle, replace=replace)\n",
        "\n",
        "        sequence_list.append(sequence)\n",
        "\n",
        "    return sequence_list\n",
        "\n",
        "def setup_model(hp, callback_output_path):\n",
        "    early_stopper = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=15,\n",
        "        verbose=1,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "\n",
        "    # csv_logger = CSVLogger(fr'{base_output_folder}/callback_logs/{run_prefix}.csv')\n",
        "    csv_logger = CSVLogger(f'{callback_output_path}')\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=hp.init_learning_rate,\n",
        "        decay_steps=hp.decay_steps,\n",
        "        decay_rate=hp.lr_decay)\n",
        "    try:\n",
        "        opt = opt_dict[hp.opt](learning_rate=lr_schedule)\n",
        "    except Exception:\n",
        "        print(f\"{hp.opt} does not work with scheduler\")\n",
        "        opt = opt_dict[hp.opt](learning_rate=hp.init_learning_rate)\n",
        "\n",
        "    model_maker = model_maker_dict[hp.schematic]\n",
        "\n",
        "    model = model_maker(hp.lookback, len(hp.features), hp.lob_unit, hp.inception_unit, hp.lstm_unit)\n",
        "\n",
        "    model.compile(loss=loss_dict[hp.loss], optimizer=opt, metrics=[], weighted_metrics=[])\n",
        "\n",
        "    callback_list = [early_stopper, csv_logger]\n",
        "\n",
        "    return model, callback_list\n",
        "\n",
        "def plot_pnl(output_list, sequence_list, chunk_list):\n",
        "    wealth_factor_list = []\n",
        "\n",
        "    for sq, chunk in zip(sequence_list, chunk_list):\n",
        "        idx = sq.get_target_indices(0)\n",
        "        wealth_factor = chunk.label.reindex(idx)\n",
        "        wealth_factor_list.append(wealth_factor)\n",
        "\n",
        "    wealth_df = pd.concat(wealth_factor_list)\n",
        "    wealth_df[['pred_short', 'pred_neutral', 'pred_long']] = np.vstack(output_list)\n",
        "    wealth_df = wealth_df.sort_index()\n",
        "    pred_df = wealth_df[['pred_short', 'pred_neutral', 'pred_long']]\n",
        "\n",
        "    wealth_df['trade_returns'] = 1\n",
        "    wealth_df.loc[pred_df['pred_short'] == pred_df.max(1), 'trade_returns'] = wealth_df.loc[pred_df['pred_short'] == pred_df.max(1), 'short_wealth']\n",
        "    wealth_df.loc[pred_df['pred_long'] == pred_df.max(1), 'trade_returns'] = wealth_df.loc[pred_df['pred_long'] == pred_df.max(1), 'long_wealth']\n",
        "    wealth_df['trade_returns'] = wealth_df['trade_returns'] - 1\n",
        "\n",
        "    to_plot = wealth_df['trade_returns'].cumsum()\n",
        "    to_plot.index = to_plot.index.astype(str)\n",
        "    return wealth_df, to_plot.plot()\n",
        "\n",
        "def plot_save_pnl(sequence_list, chunk_list, count_str, prefix, save=True):\n",
        "    confusion = pd.DataFrame(0, \n",
        "                            index=['true_short', 'true_neutral', 'true_long'],\n",
        "                            columns=['short', 'neutral', 'long'])\n",
        "    output_list = []\n",
        "    for sq in sequence_list:\n",
        "        for X, y in sq:\n",
        "            output = model.predict(X, verbose=1)\n",
        "            confusion += get_confusion(y, output)\n",
        "            output_list.append(output)\n",
        "    print(confusion)\n",
        "    print(confusion/confusion.sum(0))\n",
        "    if save:\n",
        "        pd.concat([confusion, confusion/confusion.sum()]).to_csv(f'{base_output_folder}/run_summary/{run_prefix}/{prefix}_confusion_{count_str}.csv')\n",
        "\n",
        "    wealth_df, ax = plot_pnl(output_list, sequence_list, chunk_list)\n",
        "\n",
        "    if save:\n",
        "        wealth_df.to_csv(f'{base_output_folder}/output/{run_prefix}/{prefix}_pnl_df_{count_str}.csv')\n",
        "        ax.get_figure().savefig(f'{base_output_folder}/plots/{run_prefix}/{prefix}_pnl_{count_str}.png')\n",
        "    plt.show()\n",
        "\n",
        "    return wealth_df\n",
        "\n",
        "def model_writer(input_str, file_handler):\n",
        "    input_lines = input_str.split('\\n')\n",
        "    for l in input_lines:\n",
        "        file_handler.writelines(l)\n",
        "        file_handler.write('\\n')\n",
        "\n",
        "def trend_index(short, neutral, long, chunk, random_neutral=True):\n",
        "    is_up = chunk.label['long_wealth'].sort_values().dropna() > 1\n",
        "    is_down = chunk.label['short_wealth'].sort_values().dropna() > 1\n",
        "    sideways = chunk.label.dropna().sum(1).sort_values().loc[~(is_up | is_down)]\n",
        "\n",
        "    if random_neutral:\n",
        "        selected_side = np.random.choice(sideways.index, neutral, replace=False)\n",
        "    else:\n",
        "        selected_side = sideways.iloc[:neutral].index\n",
        "\n",
        "    selected_up = is_up[is_up].iloc[is_up.sum()-long:].index\n",
        "    selected_down = is_down[is_down].iloc[is_down.sum()-short:].index\n",
        "\n",
        "    return pd.Index(np.hstack([selected_side, selected_up, selected_down])).sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6d7T11ZmQv7",
        "outputId": "2194130f-98cd-4dbb-d6b4-92703adb576e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_count=0\n",
            "HyperParameters(features=['bid_1', 'bid_size_1', 'ask_1', 'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3', 'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4', 'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5'], lookback=50, epochs=300, cv=5, batch_size=256, batch_no=(None, None, None), shuffle=True, init_learning_rate=0.0001, seed=420, lr_decay=1.0, trim='both', decay_steps=2000, schematic='adhoc', opt='Adam', replace=False, loss='fc', training_week=8, lob_unit=24, inception_unit=24, lstm_unit=48, trend_proportion=(0.16666666666666666, 0.16666666666666666, 0.16666666666666666), min_proportion=False, randomize_neutral=True, skip_to='2022-11-01 00:00:00.000', end='2023-03-01 00:00:00')\n",
            "count_str='00033'\n",
            "Data loading started\n",
            "Epoch 1/300\n",
            "493/493 [==============================] - 19s 27ms/step - loss: 1.0575 - val_loss: 0.6533\n",
            "Epoch 2/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.4345 - val_loss: 0.2899\n",
            "Epoch 3/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.2057 - val_loss: 0.1602\n",
            "Epoch 4/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.1273 - val_loss: 0.1184\n",
            "Epoch 5/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.1034 - val_loss: 0.1060\n",
            "Epoch 6/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0960 - val_loss: 0.1021\n",
            "Epoch 7/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0933 - val_loss: 0.1017\n",
            "Epoch 8/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0922 - val_loss: 0.0997\n",
            "Epoch 9/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0915 - val_loss: 0.0996\n",
            "Epoch 10/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0910 - val_loss: 0.0986\n",
            "Epoch 11/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0906 - val_loss: 0.0981\n",
            "Epoch 12/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0903 - val_loss: 0.1009\n",
            "Epoch 13/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0902 - val_loss: 0.0981\n",
            "Epoch 14/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0901 - val_loss: 0.0975\n",
            "Epoch 15/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0898 - val_loss: 0.0976\n",
            "Epoch 16/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0897 - val_loss: 0.0978\n",
            "Epoch 17/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0896 - val_loss: 0.0974\n",
            "Epoch 18/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0894 - val_loss: 0.0980\n",
            "Epoch 19/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0893 - val_loss: 0.1016\n",
            "Epoch 20/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0893 - val_loss: 0.0978\n",
            "Epoch 21/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0891 - val_loss: 0.0997\n",
            "Epoch 22/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0890 - val_loss: 0.0979\n",
            "Epoch 23/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0890 - val_loss: 0.0974\n",
            "Epoch 24/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0888 - val_loss: 0.1013\n",
            "Epoch 25/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0887 - val_loss: 0.0973\n",
            "Epoch 26/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0886 - val_loss: 0.0988\n",
            "Epoch 27/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0885 - val_loss: 0.0977\n",
            "Epoch 28/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0883 - val_loss: 0.0975\n",
            "Epoch 29/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0883 - val_loss: 0.0992\n",
            "Epoch 30/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0887 - val_loss: 0.0986\n",
            "Epoch 31/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0881 - val_loss: 0.0982\n",
            "Epoch 32/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0883 - val_loss: 0.0974\n",
            "Epoch 33/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0881 - val_loss: 0.0976\n",
            "Epoch 34/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0884 - val_loss: 0.0993\n",
            "Epoch 35/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0882 - val_loss: 0.0991\n",
            "Epoch 36/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0879 - val_loss: 0.1036\n",
            "Epoch 37/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0881 - val_loss: 0.0968\n",
            "Epoch 38/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0878 - val_loss: 0.0972\n",
            "Epoch 39/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0877 - val_loss: 0.1013\n",
            "Epoch 40/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0878 - val_loss: 0.1006\n",
            "Epoch 41/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0878 - val_loss: 0.0997\n",
            "Epoch 42/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0880 - val_loss: 0.0980\n",
            "Epoch 43/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0876 - val_loss: 0.0999\n",
            "Epoch 44/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0876 - val_loss: 0.0992\n",
            "Epoch 45/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0876 - val_loss: 0.0977\n",
            "Epoch 46/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0873 - val_loss: 0.0971\n",
            "Epoch 47/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0876 - val_loss: 0.0973\n",
            "Epoch 48/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0874 - val_loss: 0.0988\n",
            "Epoch 49/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0873 - val_loss: 0.0969\n",
            "Epoch 50/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0874 - val_loss: 0.1076\n",
            "Epoch 51/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0874 - val_loss: 0.0965\n",
            "Epoch 52/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0870 - val_loss: 0.1010\n",
            "Epoch 53/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0873 - val_loss: 0.0972\n",
            "Epoch 54/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0872 - val_loss: 0.1010\n",
            "Epoch 55/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0869 - val_loss: 0.0972\n",
            "Epoch 56/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0870 - val_loss: 0.0967\n",
            "Epoch 57/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0870 - val_loss: 0.0968\n",
            "Epoch 58/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0868 - val_loss: 0.0995\n",
            "Epoch 59/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0866 - val_loss: 0.0992\n",
            "Epoch 60/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0870 - val_loss: 0.0975\n",
            "Epoch 61/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0868 - val_loss: 0.1010\n",
            "Epoch 62/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0871 - val_loss: 0.1010\n",
            "Epoch 63/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0866 - val_loss: 0.0964\n",
            "Epoch 64/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0866 - val_loss: 0.0976\n",
            "Epoch 65/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0861 - val_loss: 0.1058\n",
            "Epoch 66/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0868 - val_loss: 0.1010\n",
            "Epoch 67/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0868 - val_loss: 0.0979\n",
            "Epoch 68/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0866 - val_loss: 0.0971\n",
            "Epoch 69/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0863 - val_loss: 0.0970\n",
            "Epoch 70/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0861 - val_loss: 0.0971\n",
            "Epoch 71/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0866 - val_loss: 0.0964\n",
            "Epoch 72/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0862 - val_loss: 0.0964\n",
            "Epoch 73/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0861 - val_loss: 0.1013\n",
            "Epoch 74/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0864 - val_loss: 0.0978\n",
            "Epoch 75/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0863 - val_loss: 0.0973\n",
            "Epoch 76/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0861 - val_loss: 0.0976\n",
            "Epoch 77/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0865 - val_loss: 0.0962\n",
            "Epoch 78/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0862 - val_loss: 0.0975\n",
            "Epoch 79/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0863 - val_loss: 0.0985\n",
            "Epoch 80/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0861 - val_loss: 0.0979\n",
            "Epoch 81/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0860 - val_loss: 0.0973\n",
            "Epoch 82/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0860 - val_loss: 0.0964\n",
            "Epoch 83/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0860 - val_loss: 0.0982\n",
            "Epoch 84/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0859 - val_loss: 0.0981\n",
            "Epoch 85/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0859 - val_loss: 0.0975\n",
            "Epoch 86/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0856 - val_loss: 0.0964\n",
            "Epoch 87/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0860 - val_loss: 0.0962\n",
            "Epoch 88/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0862 - val_loss: 0.1006\n",
            "Epoch 89/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0860 - val_loss: 0.0985\n",
            "Epoch 90/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0860 - val_loss: 0.0966\n",
            "Epoch 91/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0857 - val_loss: 0.0978\n",
            "Epoch 92/300\n",
            "493/493 [==============================] - 13s 27ms/step - loss: 0.0858 - val_loss: 0.0961\n",
            "Epoch 93/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0856 - val_loss: 0.0982\n",
            "Epoch 94/300\n",
            "493/493 [==============================] - 13s 25ms/step - loss: 0.0855 - val_loss: 0.0960\n",
            "Epoch 95/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0857 - val_loss: 0.0964\n",
            "Epoch 96/300\n",
            "493/493 [==============================] - 12s 25ms/step - loss: 0.0857 - val_loss: 0.1002\n",
            "Epoch 97/300\n",
            "493/493 [==============================] - 13s 26ms/step - loss: 0.0857 - val_loss: 0.0978\n",
            "Epoch 98/300\n",
            "305/493 [=================>............] - ETA: 4s - loss: 0.0840"
          ]
        }
      ],
      "source": [
        "\n",
        "continue_loop = 0\n",
        "end_loop = 9999\n",
        "digit = 5\n",
        "\n",
        "for run_count, hp in enumerate(hp_list[continue_loop:]):\n",
        "    \n",
        "    np.random.seed(hp.seed)\n",
        "    run_count += continue_loop\n",
        "    if run_count >= end_loop:\n",
        "        break\n",
        "    print(f'{run_count=}')\n",
        "    print(hp)\n",
        "    counter = get_count(base_output_folder)\n",
        "    count_str = f'{counter:05}'\n",
        "    print(f\"{count_str=}\")\n",
        "\n",
        "    with open(fr\"{base_output_folder}/run_summary/{run_prefix}/hp_{count_str}.json\", 'w+') as f:\n",
        "        json.dump(asdict(hp), f)\n",
        "\n",
        "    if hp.trim == 'both':\n",
        "        hp.trim = (hp.lookback, hp.lookback)\n",
        "    \n",
        "    chunk_list_gen = get_chunk_list_gen(hp.training_week+1, skip_to=pd.Timestamp(hp.skip_to), end=pd.Timestamp(hp.end))\n",
        "\n",
        "    conf_list = []\n",
        "\n",
        "    for i, chunk_list in enumerate(chunk_list_gen):\n",
        "        break\n",
        "\n",
        "    train_chunk = concat_chunk(*chunk_list[:hp.training_week])\n",
        "    validation_chunk = chunk_list[-1]\n",
        "\n",
        "    morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "    morning_val_chunk, afternoon_val_chunk, night_val_chunk = split_market(validation_chunk)\n",
        "\n",
        "    up_select = int((train_chunk.label['long_wealth'] > 1).sum() * hp.trend_proportion[2])\n",
        "    side_select = int((train_chunk.label.max(1) < 1).sum() * hp.trend_proportion[1])\n",
        "    down_select = int((train_chunk.label['short_wealth'] > 1).sum() * hp.trend_proportion[0])\n",
        "    if hp.min_proportion:\n",
        "        equal = min(s for s in [up_select, side_select, down_select] if s > 0)\n",
        "        up_select = side_select = down_select = equal\n",
        "\n",
        "    scaler = fit_to_chunk_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, StandardScaler())\n",
        "\n",
        "    sequence_list = get_sequence_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, scaler, epsilon=1.000, \n",
        "                    lookback=hp.lookback, shuffle=False, batch_size=hp.batch_size)\n",
        "\n",
        "    up_trend = trend_index(down_select, side_select, up_select, train_chunk, random_neutral=hp.randomize_neutral)\n",
        "\n",
        "    sequence_list = [stride_data.SubSequence(sq, up_trend) for sq in sequence_list]\n",
        "\n",
        "    in_sample_list = get_sequence_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, scaler, \n",
        "                                    epsilon=1.000, lookback=hp.lookback, batch_size=9999999)\n",
        "    \n",
        "    in_sample_list = [stride_data.SubSequence(sq, up_trend, add_last_batch=True) for sq in in_sample_list]\n",
        "\n",
        "    validation_list = get_sequence_list([morning_val_chunk, afternoon_val_chunk, night_val_chunk], hp.features, scaler, \n",
        "                                     epsilon=1.000, lookback=hp.lookback, batch_size=9999999)\n",
        "    \n",
        "    up_select = int((validation_chunk.label['long_wealth'] > 1).sum() * hp.trend_proportion[2])\n",
        "    side_select = int((validation_chunk.label.max(1) < 1).sum() * hp.trend_proportion[1])\n",
        "    down_select = int((validation_chunk.label['short_wealth'] > 1).sum() * hp.trend_proportion[0])\n",
        "    if hp.min_proportion:\n",
        "        equal = min(s for s in [up_select, side_select, down_select] if s > 0)\n",
        "        up_select = side_select = down_select = equal\n",
        "\n",
        "    up_trend = trend_index(down_select, side_select, up_select, validation_chunk, random_neutral=hp.randomize_neutral)\n",
        "\n",
        "    sub_validation_list = [stride_data.SubSequence(sq, up_trend, add_last_batch=True) for sq in validation_list]\n",
        "    # break\n",
        "    sequence = stride_data.CombinedSequence(*sequence_list, shuffle=hp.shuffle, seed=hp.seed)\n",
        "    in_sample = stride_data.CombinedSequence(*in_sample_list, shuffle=False)\n",
        "    validation = stride_data.CombinedSequence(*validation_list, shuffle=False)\n",
        "    sub_validation = stride_data.CombinedSequence(*sub_validation_list, shuffle=False)\n",
        "\n",
        "    tf.keras.utils.set_random_seed(hp.seed)\n",
        "    model, callback_list = setup_model(hp, fr'{base_output_folder}/callback_logs/{run_prefix}/loss_{0}.csv')\n",
        "    model.build(input_shape=(None, hp.lookback, len(hp.features)))\n",
        "\n",
        "    model.fit(x=sequence,\n",
        "            use_multiprocessing=False,\n",
        "            validation_data=sub_validation,\n",
        "            epochs=hp.epochs,\n",
        "            verbose=1,\n",
        "            callbacks=callback_list)\n",
        "    \n",
        "    plot_save_pnl(in_sample_list, [morning_train_chunk, afternoon_train_chunk, night_train_chunk], count_str, prefix='in_sample', save=True)\n",
        "    plot_save_pnl(validation_list, [morning_val_chunk, afternoon_val_chunk, night_val_chunk], count_str, prefix='validation', save=True)\n",
        "    plot_save_pnl(sub_validation_list, [morning_val_chunk, afternoon_val_chunk, night_val_chunk], count_str, prefix='sub_validation', save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmHTog5eP6bI"
      },
      "outputs": [],
      "source": [
        "sequence = stride_data.CombinedSequence(*sequence_list, shuffle=hp.shuffle, seed=hp.seed)\n",
        "in_sample = stride_data.CombinedSequence(*in_sample_list, shuffle=False)\n",
        "validation = stride_data.CombinedSequence(*validation_list, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Ioo-qwWNaB",
        "outputId": "ca463684-d929-4eb3-94f0-dfce5680b9c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.56985871e-04, 6.59968603e-01, 3.39874411e-01])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation[2][1].mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpgEY6QszzpT"
      },
      "outputs": [],
      "source": [
        "up_select = int((train_chunk.label['long_wealth'] > 1).sum() / 2)\n",
        "side_select = int((train_chunk.label.max(1) < 1).sum() / 2)\n",
        "down_select = int((train_chunk.label['short_wealth'] > 1).sum() / 2)\n",
        "equal = min(up_select, side_select, down_select)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dup9zT2_7YuE"
      },
      "outputs": [],
      "source": [
        "scaler = fit_to_chunk_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, StandardScaler())\n",
        "\n",
        "sequence_list = get_sequence_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, scaler, epsilon=1.000, \n",
        "                   lookback=hp.lookback, shuffle=False, batch_size=hp.batch_size)\n",
        "\n",
        "up_trend = trend_index(equal,equal,equal, train_chunk, random_neutral=True)\n",
        "\n",
        "sequence_list = [stride_data.SubSequence(sq, up_trend) for sq in sequence_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYaucrXPlVJT"
      },
      "outputs": [],
      "source": [
        "in_sample_list = get_sequence_list([morning_train_chunk, afternoon_train_chunk, night_train_chunk], hp.features, scaler, \n",
        "                                    epsilon=1.000, lookback=hp.lookback, batch_size=9999999)\n",
        "# in_sample_list = [stride_data.SubSequence(sq, up_trend, add_last_batch=True) for sq in in_sample_list]\n",
        "\n",
        "# val_up = (validation_chunk.label['long_wealth'] > 1).sum()\n",
        "\n",
        "# up_trend_val = trend_index(0,val_up,val_up, validation_chunk)\n",
        "\n",
        "validation_list = get_sequence_list([morning_val_chunk, afternoon_val_chunk, night_val_chunk], hp.features, scaler, \n",
        "                                     epsilon=1.000, lookback=hp.lookback, batch_size=9999999)\n",
        "# validation_list = [stride_data.SubSequence(sq, up_trend_val, add_last_batch=True) for sq in validation_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNLSjCl7oTP3"
      },
      "outputs": [],
      "source": [
        "sequence = stride_data.CombinedSequence(*sequence_list, shuffle=hp.shuffle, seed=hp.seed)\n",
        "in_sample = stride_data.CombinedSequence(*in_sample_list, shuffle=False)\n",
        "validation = stride_data.CombinedSequence(*validation_list, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4XqxmWWVP0a",
        "outputId": "203535c9-5020-4f9a-9395-1fb4c3b6dec6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.13430071, 0.86569929, 0.        ])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "in_sample_list[0][0][1].mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WIKCA6gzvOUj",
        "outputId": "8cad57bc-78e8-4c85-f501-82c191fafb57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "608/608 [==============================] - 2s 3ms/step\n",
            "360/360 [==============================] - 1s 3ms/step\n",
            "1531/1531 [==============================] - 4s 3ms/step\n",
            "              short  neutral  long\n",
            "true_short      0.0    15444   0.0\n",
            "true_neutral    0.0    64461   0.0\n",
            "true_long       0.0        5   0.0\n",
            "              short   neutral  long\n",
            "true_short      NaN  0.193267   NaN\n",
            "true_neutral    NaN  0.806670   NaN\n",
            "true_long       NaN  0.000063   NaN\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85/85 [==============================] - 0s 3ms/step\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "194/194 [==============================] - 1s 3ms/step\n",
            "              short  neutral  long\n",
            "true_short      0.0     2458   0.0\n",
            "true_neutral    0.0     7924   0.0\n",
            "true_long       0.0        0   0.0\n",
            "              short   neutral  long\n",
            "true_short      NaN  0.236756   NaN\n",
            "true_neutral    NaN  0.763244   NaN\n",
            "true_long       NaN  0.000000   NaN\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-34b8c3d72b70>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplot_save_pnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_sample_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmorning_train_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafternoon_train_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnight_train_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'in_sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_save_pnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmorning_val_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafternoon_val_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnight_val_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# plot_save_pnl(valida, uptrend_chunk_list, count_str, prefix='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c20c6665927b>\u001b[0m in \u001b[0;36mplot_save_pnl\u001b[0;34m(sequence_list, chunk_list, count_str, prefix, save)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base_output_folder}/run_summary/{run_prefix}/{prefix}_confusion_{count_str}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mwealth_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_pnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c20c6665927b>\u001b[0m in \u001b[0;36mplot_pnl\u001b[0;34m(output_list, sequence_list, chunk_list)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mwealth_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mwealth_factor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwealth_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/stride_data.py\u001b[0m in \u001b[0;36mget_target_indices\u001b[0;34m(self, input_idx)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mindices_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mindices_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/stride_data.py\u001b[0m in \u001b[0;36mget_target_indices\u001b[0;34m(self, idx_input)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_target_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0midx_range_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_idx_range_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_range_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/stride_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_target_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0midx_range_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_idx_range_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_range_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5337\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5339\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5340\u001b[0m         \u001b[0;31m# Because we ruled out integer above, we always get an arraylike here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;31m# At this point we know the result is an array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatetimeLikeArrayT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_getitem_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m_get_getitem_freq\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0mto\u001b[0m \u001b[0massign\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0m__getitem__\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mis_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_period_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_period\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_period_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/dtypes.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "plot_save_pnl(in_sample_list, [morning_train_chunk, afternoon_train_chunk, night_train_chunk], '0', prefix='in_sample', save=False)\n",
        "plot_save_pnl(validation_list, [morning_val_chunk, afternoon_val_chunk, night_val_chunk], '0', prefix='validation', save=False)\n",
        "# plot_save_pnl(valida, uptrend_chunk_list, count_str, prefix='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKcx-WqXMoH"
      },
      "outputs": [],
      "source": [
        "# sq = sequence_list[0]\n",
        "# selected_idx = trend_index(0,100000,100000, train_chunk)\n",
        "\n",
        "# class SubSequence(Sequence):\n",
        "#     def __init__(self, sd: stride_data.StrideData, selected_idx):\n",
        "#         self.batch_size = sd.batch_size\n",
        "#         self.sd = sd\n",
        "\n",
        "#         self.key_dict = {}\n",
        "#         query_dict = {}\n",
        "#         batch_count = 0\n",
        "#         sequence_count = 0\n",
        "\n",
        "#         for i in range(len(sd)):\n",
        "#             target_idx = sd.get_target_indices(i)\n",
        "#             selected_bool = pd.Index(target_idx).isin(selected_idx)\n",
        "#             batch_count += selected_bool.sum()\n",
        "\n",
        "#             if batch_count < self.batch_size:\n",
        "#                 query_dict[i] = np.argwhere(selected_bool)[:,0]\n",
        "            \n",
        "#             else:\n",
        "#                 batch_count -= self.batch_size\n",
        "#                 query_arr = np.argwhere(selected_bool)[:,0]\n",
        "#                 cut = query_arr.shape[0] - batch_count\n",
        "\n",
        "#                 query_dict[i] = query_arr[:cut]\n",
        "#                 self.key_dict[sequence_count] = query_dict\n",
        "\n",
        "#                 query_dict = {}\n",
        "#                 query_dict[i] = query_arr[cut:]\n",
        "#                 sequence_count += 1\n",
        "\n",
        "#     def get_query_dict(self, input_idx):\n",
        "#         if input_idx < 0:\n",
        "#             input_idx = len(self.key_dict) - input_idx\n",
        "#         return self.key_dict[input_idx]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         len(self.key_dict)\n",
        "\n",
        "#     def __getitem__(self, input_idx):\n",
        "#         query_dict = self.get_query_dict(input_idx)\n",
        "#         X_list = []\n",
        "#         y_list = []\n",
        "#         for k, query in query_dict.items():\n",
        "#             print(query)\n",
        "#             X, y = self.sd[k]\n",
        "#             X_list.append(X[query])\n",
        "#             y_list.append(y[query])\n",
        "#             print(y[query].mean(0))\n",
        "#         return np.vstack(X_list), np.vstack(y_list)\n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         self.sd.on_epoch_end()\n",
        "\n",
        "#     def get_target_indices(self, input_idx):\n",
        "#         query_dict = self.get_query_dict(input_idx)\n",
        "#         indices_list = []\n",
        "#         for k, query in query_dict.items():\n",
        "#             idx = pd.Index(self.sd.get_target_indices(k))[query]\n",
        "#             indices_list.append(idx.values)\n",
        "\n",
        "#         return np.hstack(indices_list)\n",
        "\n",
        "# uptrend_sq = SubSequence(sq, selected_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBkYJocwajuc"
      },
      "outputs": [],
      "source": [
        "y = uptrend_sq[8][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7rKF04lYLaG"
      },
      "outputs": [],
      "source": [
        "uptrend_sq.sd[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU03sHXaGu3o"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_save_pnl(wf_sequence_list, uptrend_chunk_list_wf, count_str, prefix='validation_early')\n",
        "plot_save_pnl(big_batch_sequence_list, uptrend_chunk_list, count_str, prefix='train_early')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7qRnZLGpx7"
      },
      "outputs": [],
      "source": [
        "output_list = []\n",
        "for sq in big_batch_sequence_list:\n",
        "    for X, y in sq:\n",
        "        output = model.predict(X, verbose=1)\n",
        "        confusion += get_confusion(y, output)\n",
        "        output_list.append(output)\n",
        "print(confusion)\n",
        "print(confusion/confusion.sum(0))\n",
        "plot_pnl(output_list, big_batch_sequence_list, uptrend_chunk_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NIpn58bcZk9"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pavAQplmQv7"
      },
      "outputs": [],
      "source": [
        "chunk_list_gen = get_chunk_list_gen(hp.training_week+1, skip_to=hp.skip_to, end=hp.end)\n",
        "\n",
        "conf_list = []\n",
        "\n",
        "for i, chunk_list in enumerate(chunk_list_gen):\n",
        "\n",
        "    train_chunk = concat_chunk(*chunk_list[:hp.training_week])\n",
        "    walk_forward_chunk = chunk_list[-1]\n",
        "\n",
        "    morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
        "    morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)\n",
        "\n",
        "    print(make_class(night_train_chunk.label, 1.0000).mean())\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    epsilon=1.0000\n",
        "\n",
        "    for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        X_list.append(X)\n",
        "        y_list.append(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(pd.concat(X_list))\n",
        "\n",
        "    sp_generator_list = []\n",
        "\n",
        "    for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
        "\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        sp_generator : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(\n",
        "            X, y, cv=hp.cv, lookback=hp.lookback, \n",
        "            batch_size=hp.batch_size, batch_no=hp.batch_no[i], \n",
        "            shuffle=hp.shuffle, trim=hp.trim, replace=hp.replace)\n",
        "\n",
        "        sp_generator_list.append(sp_generator)\n",
        "\n",
        "    wf_sequence_list = []\n",
        "\n",
        "    for i, chunk in enumerate([morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk]):\n",
        "\n",
        "        X = chunk.lob.loc[:, hp.features]\n",
        "        X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
        "        y = make_class(chunk.label, epsilon)\n",
        "        X = X.reindex(y.index)\n",
        "        y = y.reindex(X.index)\n",
        "        #transform y here\n",
        "\n",
        "        wf_sequence : stride_data.StrideData = stride_data.StrideData(X, y, lookback=hp.lookback, \n",
        "                                                batch_size=y.shape[0] - hp.lookback, batch_no=None, \n",
        "                                                shuffle=False, replace=False)\n",
        "\n",
        "        wf_sequence_list.append(wf_sequence)\n",
        "\n",
        "    early_stopper = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=50,\n",
        "        verbose=1,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=False,\n",
        "    )\n",
        "\n",
        "    # for cv_count in range(hp.cv): #just have train-validation\n",
        "    sp_list = [next(sp_gen) for sp_gen in sp_generator_list]\n",
        "    sq = stride_data.CombinedSequence(*[sp.train_sequence for sp in sp_list], shuffle=hp.shuffle, seed=hp.seed, replace=hp.replace)\n",
        "    test_tuple = np.vstack([sp.test_tuple[0] for sp in sp_list]), np.vstack([sp.test_tuple[1] for sp in sp_list])\n",
        "\n",
        "    csv_logger = CSVLogger(fr'{base_output_folder}\\callback_logs\\{run_prefix}\\_temp\\{run_prefix}.csv')\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=hp.init_learning_rate,\n",
        "        decay_steps=hp.decay_steps,\n",
        "        decay_rate=hp.lr_decay)\n",
        "    try:\n",
        "        opt = opt_dict[hp.opt](learning_rate=lr_schedule)\n",
        "    except Exception:\n",
        "        print(f\"{hp.opt} does not work with scheduler\")\n",
        "        opt = opt_dict[hp.opt](learning_rate=hp.init_learning_rate)\n",
        "\n",
        "    # model = tf.keras.Sequential()\n",
        "    # for l, p in schematic_dict[hp.schematic]:\n",
        "    #     model.add(l(**p))\n",
        "        \n",
        "    # model.add(layers.Dense(3, activation='softmax')) # add output node\n",
        "\n",
        "    model_maker = model_maker_dict[hp.schematic]\n",
        "\n",
        "    model = model_maker(hp.lookback, len(hp.features), hp.lob_unit, hp.inception_unit, hp.lstm_unit)\n",
        "\n",
        "    model.compile(loss=loss_dict[hp.loss], optimizer=opt, metrics=[], weighted_metrics=[])\n",
        "    # with tf.device('/cpu:0'):\n",
        "    model.fit(x=sq,\n",
        "            use_multiprocessing=False,\n",
        "            validation_data=test_tuple,\n",
        "            epochs=hp.epochs,\n",
        "            verbose=1,\n",
        "            callbacks=[csv_logger, early_stopper])\n",
        "\n",
        "    confusion = pd.DataFrame(0, \n",
        "                            index=['true_short', 'true_neutral', 'true_long'],\n",
        "                            columns=['short', 'neutral', 'long'])\n",
        "    for wf in wf_sequence_list:\n",
        "        output = model.predict(wf[0][0])\n",
        "        confusion += get_confusion(wf[0][1], output)\n",
        "    conf_list.append(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt0VhHFtmQv9"
      },
      "outputs": [],
      "source": [
        "print(conf_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erkTXS3zLPYf"
      },
      "outputs": [],
      "source": [
        "wf_sequence_list[0].get_target_indices(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67w68xCGTH7x"
      },
      "outputs": [],
      "source": [
        "confusion = pd.DataFrame(0, \n",
        "                        index=['true_short', 'true_neutral', 'true_long'],\n",
        "                        columns=['short', 'neutral', 'long'])\n",
        "for wf in wf_sequence_list:\n",
        "    output = model.predict(wf[0][0])\n",
        "    confusion += get_confusion(wf[0][1], output)\n",
        "\n",
        "print(confusion)\n",
        "print(confusion/confusion.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XjTpB0DcnB6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}