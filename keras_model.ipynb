{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "import ast\n",
    "import os\n",
    "from typing import Union, List\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stride_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Chunk = namedtuple('Chunk', ['lob', 'factor', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor_itr(factor_file_list=['factor_22.csv', 'factor_23.csv'], base_folder='factor',\n",
    "                   chunksize=500000):\n",
    "    itr_list = []\n",
    "    for f_name in factor_file_list:\n",
    "        itr_list.append(pd.read_csv(fr'{base_folder}/{f_name}', chunksize=chunksize))\n",
    "\n",
    "    for df_tuple in zip(*itr_list):\n",
    "        df_list = []\n",
    "        for df in df_tuple:\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "            df = df.set_index('time').sort_index()\n",
    "            # print(df.index.min(), df.index.max())\n",
    "            df = df[~df.index.duplicated()]\n",
    "            # print(df.index.min(), df.index.max())\n",
    "            df_list.append(df)\n",
    "        yield pd.concat(df_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "\n",
    "data_itr = pd.read_csv(r'data/data_night_shifted_au.csv.gz', chunksize=chunksize)\n",
    "label_itr = pd.read_csv(r'label/5_min_tp4_sl2_10yuan_target.csv', chunksize=chunksize)\n",
    "factor_itr = get_factor_itr(['factor_24.csv', 'factor_23.csv'], chunksize=chunksize)\n",
    "\n",
    "all_feature = ['volume', 'bid_1', 'bid_1', 'bid_size_1', 'ask_1',\n",
    "       'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
    "       'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
    "       'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5', 'vwap']\n",
    "\n",
    "lob_feature = ['bid_1', 'bid_size_1', 'ask_1',\n",
    "               'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3',\n",
    "               'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4',\n",
    "               'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5']\n",
    "\n",
    "def get_chunk_from_file_gen():\n",
    "    for df, fac, label in zip(data_itr, factor_itr, label_itr):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.set_index('time').sort_index()\n",
    "\n",
    "        label['time'] = pd.to_datetime(label['time'])\n",
    "        label = label.set_index('time').sort_index()\n",
    "\n",
    "        df = df.loc[~df.index.duplicated()]\n",
    "        fac = fac.loc[~fac.index.duplicated()]\n",
    "        label = label.loc[~label.index.duplicated()]\n",
    "        # print(df.index.max(), label.index.max(), fac.index.max())\n",
    "        if (fac.index.max() != label.index.max()) or (fac.index.max() != df.index.max()):\n",
    "            raise RuntimeError(\"Index do not match!\")\n",
    "\n",
    "        yield Chunk(df, fac, label), label.index.max()\n",
    "\n",
    "def concat_chunk(*chunk_tuple):\n",
    "    arg_list = []\n",
    "    for df_tuple in zip(*chunk_tuple):\n",
    "        df = pd.concat(df_tuple).sort_index()\n",
    "        df = df.loc[~df.index.duplicated()]\n",
    "        arg_list.append(df)\n",
    "    return Chunk(*arg_list)\n",
    "\n",
    "def get_weekly_data_gen(skip_to=None):\n",
    "    print('Data loading started')\n",
    "    while True:\n",
    "        chunk_gen = get_chunk_from_file_gen()\n",
    "        chunk, latest = next(chunk_gen)\n",
    "        print(f\"                                         \", end='\\r')\n",
    "        time.sleep(0)\n",
    "        print(f\"{latest=}\", end='\\r')\n",
    "        time.sleep(0)\n",
    "        if skip_to is not None:\n",
    "            if latest < skip_to:\n",
    "                continue\n",
    "\n",
    "        if (chunk.label.index.day_of_week == 0).sum() > 0:\n",
    "            start = chunk.label[chunk.label.index.day_of_week == 0].index.date.min()\n",
    "            start = pd.Timestamp(start)\n",
    "            end = start + pd.Timedelta(5,'d') #end on friday midnight\n",
    "            break\n",
    "\n",
    "    chunk = Chunk(*[data[data.index >= start] for data in chunk])\n",
    "    for new_chunk, latest in chunk_gen:\n",
    "        chunk = concat_chunk(chunk, new_chunk)\n",
    "        while latest > end:\n",
    "            print(f\"                                             \", end='\\r')\n",
    "            time.sleep(0)\n",
    "            print(f\"{latest=}\", end='\\r')\n",
    "            time.sleep(0)\n",
    "            to_yield = Chunk(*(df.loc[start:end] for df in chunk))\n",
    "            chunk = Chunk(*(df.loc[end:] for df in chunk))\n",
    "            start = start + pd.Timedelta(7,'d')\n",
    "            end = end + pd.Timedelta(7,'d')\n",
    "            yield to_yield\n",
    "\n",
    "def get_chunk_list_gen(list_len=2, skip_to=None, end=None):\n",
    "    out_list = []\n",
    "    weekly_gen = get_weekly_data_gen(skip_to)\n",
    "    for week_chunk in weekly_gen:\n",
    "        if (end is not None) and (week_chunk.label.index.max() > end):\n",
    "            break \n",
    "        out_list.append(week_chunk)\n",
    "        if len(out_list) < list_len:\n",
    "            continue\n",
    "        if len(out_list) > list_len:\n",
    "            out_list = out_list[1:] #basically, pop left\n",
    "        yield out_list\n",
    "\n",
    "def split_market(chunk, avoid_market_edge = timedelta(minutes=5)):\n",
    "    chunk_arg = []\n",
    "    for df in chunk:\n",
    "        night_df = df.loc[(df.index.time >= (datetime(1970,1,1,21,0,0)+avoid_market_edge).time()) \n",
    "                        | (df.index.time <= (datetime(1970,1,1,2,30,0)-avoid_market_edge).time())] #night market\n",
    "        morning_df = df.loc[(df.index.time >= (datetime(1970,1,1,9,0,0)+avoid_market_edge).time()) \n",
    "                            & (df.index.time <= (datetime(1970,1,1,11,30,0)-avoid_market_edge).time())] #morning market\n",
    "        afternoon_df = df.loc[(df.index.time >= (datetime(1970,1,1,13,30,0)+avoid_market_edge).time())\n",
    "                            & (df.index.time <= (datetime(1970,1,1,15,0,0)-avoid_market_edge).time())] #afternoon market\n",
    "        chunk_arg.append((morning_df, afternoon_df, night_df))\n",
    "\n",
    "    return tuple(Chunk(*df_list) for df_list in zip(*chunk_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_week = 8\n",
    "# skip_to = pd.Timestamp('2022-11-01 00:00:00.000') #use this to skip to the period we want\n",
    "# # skip_to = None\n",
    "# # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
    "# end = pd.Timestamp('2023-03-01 00:00:00')\n",
    "\n",
    "# chunk_list_gen = get_chunk_list_gen(training_week+1, skip_to=skip_to, end=end)\n",
    "# for i, chunk_list in enumerate(chunk_list_gen):\n",
    "#     break\n",
    "\n",
    "# train_chunk = concat_chunk(*chunk_list[:training_week])\n",
    "# walk_forward_chunk = chunk_list[-1]\n",
    "\n",
    "# morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
    "# morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookback = 50 #this number of timestep per batch\n",
    "# batch_size = 1000 #this number of samples in a batch\n",
    "\n",
    "# chunk = morning_train_chunk\n",
    "\n",
    "# sp_list_list = []\n",
    "# for chunk in [morning_train_chunk, afternoon_train_chunk, night_train_chunk]:\n",
    "\n",
    "#     X = chunk.lob.loc[:, all_feature].join(chunk.factor)\n",
    "#     y = chunk.label\n",
    "\n",
    "#     sp_list : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(X, y.iloc[:, 0], cv=4, lookback=lookback, \n",
    "#                                             batch_size=batch_size, batch_no=None, \n",
    "#                                             shuffle=False)\n",
    "\n",
    "#     sp_list_list.append(sp_list)\n",
    "\n",
    "# sequence_pair_list_per_cv = list(zip(*sp_list_list))\n",
    "# sequence_list_per_cv = [stride_data.CombinedSequence(*[sequence_pair_list_per_cv[i][j].train_sequence \n",
    "#                         for j in range(len(sequence_pair_list_per_cv[i]))])\n",
    "#                         for i in range(len(sequence_pair_list_per_cv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv2D in module keras.src.layers.convolutional.conv2d:\n",
      "\n",
      "class Conv2D(keras.src.layers.convolutional.base_conv.Conv)\n",
      " |  Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  2D convolution layer (e.g. spatial convolution over images).\n",
      " |  \n",
      " |  This layer creates a convolution kernel that is convolved\n",
      " |  with the layer input to produce a tensor of\n",
      " |  outputs. If `use_bias` is True,\n",
      " |  a bias vector is created and added to the outputs. Finally, if\n",
      " |  `activation` is not `None`, it is applied to the outputs as well.\n",
      " |  \n",
      " |  When using this layer as the first layer in a model,\n",
      " |  provide the keyword argument `input_shape`\n",
      " |  (tuple of integers or `None`, does not include the sample axis),\n",
      " |  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n",
      " |  in `data_format=\"channels_last\"`. You can use `None` when\n",
      " |  a dimension has variable size.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  >>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n",
      " |  >>> # size is 4.\n",
      " |  >>> input_shape = (4, 28, 28, 3)\n",
      " |  >>> x = tf.random.normal(input_shape)\n",
      " |  >>> y = tf.keras.layers.Conv2D(\n",
      " |  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
      " |  >>> print(y.shape)\n",
      " |  (4, 26, 26, 2)\n",
      " |  \n",
      " |  >>> # With `dilation_rate` as 2.\n",
      " |  >>> input_shape = (4, 28, 28, 3)\n",
      " |  >>> x = tf.random.normal(input_shape)\n",
      " |  >>> y = tf.keras.layers.Conv2D(\n",
      " |  ...     2, 3,\n",
      " |  ...     activation='relu',\n",
      " |  ...     dilation_rate=2,\n",
      " |  ...     input_shape=input_shape[1:])(x)\n",
      " |  >>> print(y.shape)\n",
      " |  (4, 24, 24, 2)\n",
      " |  \n",
      " |  >>> # With `padding` as \"same\".\n",
      " |  >>> input_shape = (4, 28, 28, 3)\n",
      " |  >>> x = tf.random.normal(input_shape)\n",
      " |  >>> y = tf.keras.layers.Conv2D(\n",
      " |  ... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n",
      " |  >>> print(y.shape)\n",
      " |  (4, 28, 28, 2)\n",
      " |  \n",
      " |  >>> # With extended batch shape [4, 7]:\n",
      " |  >>> input_shape = (4, 7, 28, 28, 3)\n",
      " |  >>> x = tf.random.normal(input_shape)\n",
      " |  >>> y = tf.keras.layers.Conv2D(\n",
      " |  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n",
      " |  >>> print(y.shape)\n",
      " |  (4, 7, 26, 26, 2)\n",
      " |  \n",
      " |  \n",
      " |  Args:\n",
      " |    filters: Integer, the dimensionality of the output space (i.e. the number\n",
      " |      of output filters in the convolution).\n",
      " |    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n",
      " |      and width of the 2D convolution window. Can be a single integer to\n",
      " |      specify the same value for all spatial dimensions.\n",
      " |    strides: An integer or tuple/list of 2 integers, specifying the strides of\n",
      " |      the convolution along the height and width. Can be a single integer to\n",
      " |      specify the same value for all spatial dimensions. Specifying any stride\n",
      " |      value != 1 is incompatible with specifying any `dilation_rate` value !=\n",
      " |      1.\n",
      " |    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |      `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n",
      " |      evenly to the left/right or up/down of the input. When `padding=\"same\"`\n",
      " |      and `strides=1`, the output has the same size as the input.\n",
      " |    data_format: A string, one of `channels_last` (default) or\n",
      " |      `channels_first`.  The ordering of the dimensions in the inputs.\n",
      " |      `channels_last` corresponds to inputs with shape `(batch_size, height,\n",
      " |      width, channels)` while `channels_first` corresponds to inputs with\n",
      " |      shape `(batch_size, channels, height, width)`. If left unspecified, it\n",
      " |      uses the `image_data_format` value found in your Keras config file at\n",
      " |      `~/.keras/keras.json` (if exists) else 'channels_last'.\n",
      " |      Note that the `channels_first` format is currently not\n",
      " |      supported by TensorFlow on CPU. Defaults to 'channels_last'.\n",
      " |    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n",
      " |      dilation rate to use for dilated convolution. Can be a single integer to\n",
      " |      specify the same value for all spatial dimensions. Currently, specifying\n",
      " |      any `dilation_rate` value != 1 is incompatible with specifying any\n",
      " |      stride value != 1.\n",
      " |    groups: A positive integer specifying the number of groups in which the\n",
      " |      input is split along the channel axis. Each group is convolved\n",
      " |      separately with `filters / groups` filters. The output is the\n",
      " |      concatenation of all the `groups` results along the channel axis. Input\n",
      " |      channels and `filters` must both be divisible by `groups`.\n",
      " |    activation: Activation function to use. If you don't specify anything, no\n",
      " |      activation is applied (see `keras.activations`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix (see\n",
      " |      `keras.initializers`). Defaults to 'glorot_uniform'.\n",
      " |    bias_initializer: Initializer for the bias vector (see\n",
      " |      `keras.initializers`). Defaults to 'zeros'.\n",
      " |    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
      " |      matrix (see `keras.regularizers`).\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector (see\n",
      " |      `keras.regularizers`).\n",
      " |    activity_regularizer: Regularizer function applied to the output of the\n",
      " |      layer (its \"activation\") (see `keras.regularizers`).\n",
      " |    kernel_constraint: Constraint function applied to the kernel matrix (see\n",
      " |      `keras.constraints`).\n",
      " |    bias_constraint: Constraint function applied to the bias vector (see\n",
      " |      `keras.constraints`).\n",
      " |  \n",
      " |  Input shape:\n",
      " |    4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\n",
      " |      `data_format='channels_first'`\n",
      " |    or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\n",
      " |      `data_format='channels_last'`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\n",
      " |    `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\n",
      " |      (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\n",
      " |      and `cols` values might have changed due to padding.\n",
      " |  \n",
      " |  Returns:\n",
      " |    A tensor of rank 4+ representing\n",
      " |    `activation(conv2d(inputs, kernel) + bias)`.\n",
      " |  \n",
      " |  Raises:\n",
      " |    ValueError: if `padding` is `\"causal\"`.\n",
      " |    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv2D\n",
      " |      keras.src.layers.convolutional.base_conv.Conv\n",
      " |      keras.src.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.convolutional.base_conv.Conv:\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call. It is invoked automatically before\n",
      " |      the first execution of `call()`.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses\n",
      " |      (at the discretion of the subclass implementer).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      The `call()` method may not create state (except in its first\n",
      " |      invocation, wrapping the creation of variables or other resources in\n",
      " |      `tf.init_scope()`).  It is recommended to create state, including\n",
      " |      `tf.Variable` instances and nested `Layer` instances,\n",
      " |       in `__init__()`, or in the `build()` method that is\n",
      " |      called automatically before `call()` executes for the first time.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as\n",
      " |            tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs`\n",
      " |            only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask\n",
      " |            generated for `inputs` by the previous layer (if `input` did come\n",
      " |            from a layer that generated a corresponding mask, i.e. if it came\n",
      " |            from a Keras layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,\n",
      " |              or structure of shape tuples / `tf.TensorShape` instances\n",
      " |              (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `tf.TensorShape` instance\n",
      " |          or structure of `tf.TensorShape` instances.\n",
      " |  \n",
      " |  convolution_op(self, inputs, kernel)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of\n",
      " |      dict every time it is called. The callers should make a copy of the\n",
      " |      returned dict if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific\n",
      " |          uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid\n",
      " |          value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the\n",
      " |          constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be\n",
      " |      dependent on the inputs passed when calling a layer. Hence, when reusing\n",
      " |      the same layer on different inputs `a` and `b`, some entries in\n",
      " |      `layer.losses` may be dependent on `a` and some on `b`. This method\n",
      " |      automatically keeps track of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      The same code works in distributed training: the input to `add_loss()`\n",
      " |      is treated like a regularization loss and averaged across replicas\n",
      " |      by the training loop (both built-in `Model.fit()` and compliant custom\n",
      " |      training loops).\n",
      " |      \n",
      " |      The `add_loss` method can also be called directly on a Functional Model\n",
      " |      during construction. In this case, any loss Tensors passed to this Model\n",
      " |      must be symbolic and be able to be traced back to the model's `Input`s.\n",
      " |      These losses become part of the model's topology and are tracked in\n",
      " |      `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss\n",
      " |      references a `Variable` of one of the model's layers), you can wrap your\n",
      " |      loss in a zero-argument lambda. These losses are not tracked as part of\n",
      " |      the model's topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n",
      " |          losses may also be zero-argument callables which create a loss\n",
      " |          tensor.\n",
      " |        **kwargs: Used for backwards compatibility only.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This\n",
      " |      is because we cannot trace the metric result tensor back to the model's\n",
      " |      inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result\n",
      " |          of calling a `keras.Metric` instance, it will be aggregated by\n",
      " |          default using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and\n",
      " |      variance in a BatchNormalization layer) may be dependent on the inputs\n",
      " |      passed when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case,\n",
      " |      variable updates are run on the fly and thus do not need to be tracked\n",
      " |      for later execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use a `ResourceVariable` or not.\n",
      " |          See [this guide](\n",
      " |          https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
      " |           for more information.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set\n",
      " |          to `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n",
      " |          must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as\n",
      " |          `ON_READ`.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects,\n",
      " |          describing how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after\n",
      " |      updating a layer weights. It can be overridden to finalize any\n",
      " |      additional layer state after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with\n",
      " |      this layer as a list of NumPy arrays, which can in turn be used to load\n",
      " |      state into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which\n",
      " |      causes computations and the output to be in the compute dtype as well.\n",
      " |      This is done by the base Layer class in `Layer.__call__`, so you do not\n",
      " |      have to insert these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision\n",
      " |      when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n",
      " |      output will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Return Functional API nodes upstream of this layer.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is\n",
      " |      accessed, so it is eager safe: accessing `losses` under a\n",
      " |      `tf.GradientTape` will propagate gradients back to the corresponding\n",
      " |      variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics attached to the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are\n",
      " |      expected to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Return Functional API nodes downstream of this layer.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
      " |      not themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to\n",
      " |      enable the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input\n",
      " |      tensor of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a\n",
      " |      nicely-formatted error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layers.Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.convolutional.conv2d.Conv2D at 0x126a553af70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Conv2D(16, (1,2), (1,2), activation= 'leaky_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schematic_dict = {\n",
    "    'stride2_depth_simple': [\n",
    "        (layers.Reshape, {'target_shape': (50,-1,1)}), #50 lookback\n",
    "        (layers.Conv2D, {'filters': 16, 'kernel_size': (1, 2), 'strides': (1, 2), 'activation': 'leaky_relu'}),\n",
    "        # (layers.Conv2D, {'filters': 8, 'kernel_size': (5, 1), 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Conv2D, {'filters': 16, 'kernel_size': (1, 2), 'strides': (1, 2), 'activation': 'leaky_relu'}),\n",
    "        (layers.Conv2D, {'filters': 16, 'kernel_size': (5, 1), 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Conv2D, {'filters': 16, 'kernel_size': (1, 5), 'activation': 'leaky_relu'}),\n",
    "        # (layers.Conv2D, {'filters': 8, 'kernel_size': (5, 1), 'padding': 'same', 'activation': 'leaky_relu'}),\n",
    "        (layers.Reshape, {'target_shape': (50,-1)}), #50 lookback\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 32, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "    ],\n",
    "    'midnight_1': [ # nope\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Dense, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Dense, {'units': 5, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_2': [\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units': 5, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_3': [ # nope\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Dense, {'units': 5, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_4': [\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units': 5, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_5': [ # nope\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6': [\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_rnn': [\n",
    "        (layers.SimpleRNN, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.SimpleRNN, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_1': [\n",
    "        (layers.LSTM, {'units': 30, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_2': [\n",
    "        (layers.LSTM, {'units': 30, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_2_rnn': [\n",
    "        (layers.SimpleRNN, {'units': 30, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.SimpleRNN, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_3': [\n",
    "        (layers.LSTM, {'units': 40, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_4': [\n",
    "        (layers.LSTM, {'units': 128, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_4_mini': [\n",
    "        (layers.LSTM, {'units': 32, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_5': [\n",
    "        (layers.LSTM, {'units': 128, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_6': [\n",
    "        (layers.LSTM, {'units': 128, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2, 'return_sequences': True}),\n",
    "        (layers.Dropout, {'rate': 0.2}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2}),\n",
    "        (layers.Dropout, {'rate': 0.2}),\n",
    "        (layers.Dense, {'units':32, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_6': [\n",
    "        (layers.LSTM, {'units': 128, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2, 'return_sequences': True}),\n",
    "        (layers.Dropout, {'rate': 0.2}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2, 'return_sequences': True}),\n",
    "        (layers.Dropout, {'rate': 0.2}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2}),\n",
    "        (layers.Dropout, {'rate': 0.2}),\n",
    "        (layers.Dense, {'units':32, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_6_7': [\n",
    "        (layers.LSTM, {'units': 128, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2, 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2, 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 64, 'kernel_regularizer': 'l2', 'recurrent_dropout': 0.2}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':32, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_7': [ # nope\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.2, 'return_sequences': True}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.2}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', }),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_8': [ # nope\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.4, 'return_sequences': True}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.4}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', }),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_9': [ # out\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.5, 'return_sequences': True}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'recurrent_dropout':0.5}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', }),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'midnight_10': [\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'saturday_1': [\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'friday_1': [\n",
    "        (layers.LSTM, {'units': 25, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'friday_2': [\n",
    "        (layers.LSTM, {'units': 32, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 24, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 16, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 16, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2', 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'tuesday_1': [\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True, 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'tuesday_2': [\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'return_sequences': True}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2', 'activation': 'relu'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "#         (layers.Dense, {'units': output_count})\n",
    "    ],\n",
    "    'cnn_1':[\n",
    "        (layers.Conv1D, {'filters': 20, 'kernel_size':10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.LSTM, {'units': 20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "    ],\n",
    "    'cnn_2':[\n",
    "        (layers.Conv1D, {'filters': 20, 'kernel_size':10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Conv1D, {'filters': 20, 'kernel_size':10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Flatten, {}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "    ],\n",
    "    'cnn_3':[\n",
    "        (layers.Conv1D, {'filters': 20, 'kernel_size':10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Conv1D, {'filters': 20, 'kernel_size':10, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.Flatten, {}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':20, 'kernel_regularizer': 'l2'}),\n",
    "        (layers.BatchNormalization, {}),\n",
    "        (layers.Dense, {'units':10, 'kernel_regularizer': 'l2'}),\n",
    "    ]\n",
    "}\n",
    "\n",
    "opt_dict = {\n",
    "    'Adam': tf.keras.optimizers.Adam,\n",
    "    'Nadam': tf.keras.optimizers.Nadam,\n",
    "}\n",
    "\n",
    "# try\n",
    "\n",
    "for k, s in schematic_dict.items():\n",
    "    for l, p in s:\n",
    "        l(**p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "metric_dict = {\n",
    "    # 'mse': tf.keras.metrics.MeanSquaredError(name='mean_squared_error', dtype=None),\n",
    "    # 'mae': tf.keras.metrics.MeanAbsoluteError(name='mean_absolute_error', dtype=None),\n",
    "    'coss': tf.keras.metrics.CosineSimilarity(name='cosine_similarity', dtype=None),\n",
    "    'ce': tf.keras.metrics.CategoricalCrossentropy(name='categorical_ce', dtype=None)\n",
    "}\n",
    "\n",
    "cosine = tf.keras.losses.CosineSimilarity(name='cosine_similarity')\n",
    "mse = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
    "mae = tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error')\n",
    "\n",
    "ce = tf.keras.losses.CategoricalCrossentropy(name='categorical_ce')\n",
    "fc = tf.keras.losses.CategoricalFocalCrossentropy()\n",
    "\n",
    "loss_dict = {\n",
    "    # 'mse': tf.keras.losses.MeanSquaredError(name='mean_squared_error', reduction=\"auto\"),\n",
    "    # 'mae': tf.keras.losses.MeanAbsoluteError(name='mean_absolute_error', reduction=\"auto\"),\n",
    "    'coss': tf.keras.losses.CosineSimilarity(name='cosine_similarity', reduction=\"auto\"),\n",
    "    'ce': tf.keras.losses.CategoricalCrossentropy(name='categorical_ce', reduction='auto'),\n",
    "    'fc': tf.keras.losses.CategoricalFocalCrossentropy(),\n",
    "    # 'ce': tf.keras.losses.BinaryFocalCrossentropy(name='categorical_ce', reduction='auto'),\n",
    "    # 'coss-mse': lambda y, yhat: mse(y, yhat) + cosine(y, yhat),\n",
    "    # 'coss-mae': lambda y, yhat: mae(y, yhat) + cosine(y, yhat),\n",
    "}\n",
    "\n",
    "#           'features' :  [['High', 'Low', 'Open', 'Close', \n",
    "#                          'vix_forward_5_historical', 'vix_forward_10_historical', 'vix_forward_15_historical']],\n",
    "\n",
    "p_grid = {'batch_size' : [100],\n",
    "          'features' :  [lob_feature],\n",
    "          'init_learning_rate' : [0.25],\n",
    "          'lr_decay' : [0.95],\n",
    "          'loss': ['fc'],\n",
    "          'schematic' : [\n",
    "#                          'cnn_1',\n",
    "                        #  'cnn_2',\n",
    "#                          'cnn_3',\n",
    "#                          'tuesday_1',\n",
    "#                          'tuesday_2', \n",
    "                        #  'midnight_1',]}\n",
    "                         'stride2_depth_simple',]}\n",
    "#                          'midnight_6_2_rnn',\n",
    "                        #  'midnight_6_rnn',]}\n",
    "#                          'midnight_7',\n",
    "#                          'midnight_8',\n",
    "#                          'midnight_9',\n",
    "#                          'friday_1',\n",
    "#                          'friday_2',\n",
    "#                          'midnight_6_3',\n",
    "#                          'midnight_6_4',\n",
    "#                          'midnight_6_5',]}\n",
    "#                          'midnight_6_6',]}\n",
    "#                          'midnight_6_7']}\n",
    "\n",
    "p_grid = ParameterGrid(p_grid)\n",
    "len(p_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bid_1',\n",
       " 'bid_size_1',\n",
       " 'ask_1',\n",
       " 'ask_size_1',\n",
       " 'bid_2',\n",
       " 'bid_size_2',\n",
       " 'ask_2',\n",
       " 'ask_size_2',\n",
       " 'bid_3',\n",
       " 'bid_size_3',\n",
       " 'ask_3',\n",
       " 'ask_size_3',\n",
       " 'bid_4',\n",
       " 'bid_size_4',\n",
       " 'ask_4',\n",
       " 'ask_size_4',\n",
       " 'bid_5',\n",
       " 'bid_size_5',\n",
       " 'ask_5',\n",
       " 'ask_size_5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lob_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_if_not_exist(folder_name, base_folder='output'):\n",
    "    try:\n",
    "        os.makedirs(fr'{base_folder}\\{folder_name}')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import field, dataclass\n",
    "\n",
    "@dataclass\n",
    "class HyperParameters:\n",
    "    features: list = field(default_factory=lambda: list(all_feature))\n",
    "    lookback: int = 50\n",
    "    epochs: int = 100\n",
    "    cv: int = 4\n",
    "    batch_size: int = 100\n",
    "    batch_no: tuple = (230, 130, 520) #per market period. Mornin afternoon night.\n",
    "    shuffle: bool = True\n",
    "    init_learning_rate: float = 2.5e-1\n",
    "    seed: int = 420\n",
    "    lr_decay: float = 0.99\n",
    "    trim: str = 'both'\n",
    "    decay_steps: int = 50\n",
    "    schematic: str = 'midnight_1'\n",
    "    opt: str = 'Nadam'\n",
    "    replace: bool = False\n",
    "    loss: str = 'ce'\n",
    "    training_week: int = 8\n",
    "    # skip_to = pd.Timestamp('2022-07-02 11:24:58.000') #use this to skip to the period we want\n",
    "    skip_to: pd.Timestamp = pd.Timestamp('2022-11-01 00:00:00.000')\n",
    "    # end = pd.Timestamp('2022-07-15 23:59:59.500') #use this to fix when to stop training\n",
    "    end: pd.Timestamp = pd.Timestamp('2023-03-01 00:00:00')\n",
    "\n",
    "hp_list = [HyperParameters(**p) for p in p_grid]\n",
    "base_output_folder = 'output'\n",
    "\n",
    "run_prefix = 'test'\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "make_if_not_exist(fr\"plots\\{run_prefix}\", base_output_folder)\n",
    "make_if_not_exist(fr\"output\\{run_prefix}\", base_output_folder)\n",
    "make_if_not_exist(fr\"callback_logs\\{run_prefix}\", base_output_folder)\n",
    "make_if_not_exist(fr\"callback_logs\\{run_prefix}\\_temp\", base_output_folder)\n",
    "make_if_not_exist(fr\"run_summary\\{run_prefix}\", base_output_folder)\n",
    "    \n",
    "# if run_prefix not in [d for d in os.listdir('output\\\\') if os.path.isdir('output\\\\' + d)]:\n",
    "#     os.makedirs('output\\\\' + run_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperParameters(features=['bid_1', 'bid_size_1', 'ask_1', 'ask_size_1', 'bid_2', 'bid_size_2', 'ask_2', 'ask_size_2', 'bid_3', 'bid_size_3', 'ask_3', 'ask_size_3', 'bid_4', 'bid_size_4', 'ask_4', 'ask_size_4', 'bid_5', 'bid_size_5', 'ask_5', 'ask_size_5'], lookback=50, epochs=100, cv=4, batch_size=100, batch_no=(230, 130, 520), shuffle=True, init_learning_rate=0.25, seed=420, lr_decay=0.95, trim='both', decay_steps=50, schematic='stride2_depth_simple', opt='Nadam', replace=False, loss='fc', training_week=8, skip_to=Timestamp('2022-11-01 00:00:00'), end=Timestamp('2023-03-01 00:00:00'))\n"
     ]
    }
   ],
   "source": [
    "continue_loop = 0\n",
    "end_loop = 9999\n",
    "\n",
    "for run_count, hp in enumerate(hp_list[continue_loop:]):\n",
    "    run_count += continue_loop\n",
    "    if run_count >= end_loop:\n",
    "        break\n",
    "    print(hp)\n",
    "    if hp.trim == 'both':\n",
    "        hp.trim = (hp.lookback, hp.lookback)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading started\n",
      "latest=Timestamp('2023-01-05 09:32:26.500000')\r"
     ]
    }
   ],
   "source": [
    "chunk_list_gen = get_chunk_list_gen(hp.training_week+1, skip_to=hp.skip_to, end=hp.end)\n",
    "for i, chunk_list in enumerate(chunk_list_gen):\n",
    "    break\n",
    "\n",
    "train_chunk = concat_chunk(*chunk_list[:hp.training_week])\n",
    "walk_forward_chunk = chunk_list[-1]\n",
    "\n",
    "morning_train_chunk, afternoon_train_chunk, night_train_chunk = split_market(train_chunk)\n",
    "morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk = split_market(walk_forward_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    0.161957\n",
       " 0    0.666907\n",
       " 1    0.171136\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_class(label, epsilon=1.0001):\n",
    "    long_wins = label['long_wealth'] > label['short_wealth']\n",
    "\n",
    "    class_label = pd.Series(np.nan, index=label.index)\n",
    "    class_label[label.notna().all(1)] = 0\n",
    "    class_label[long_wins & (label['long_wealth'] > epsilon)] = 1\n",
    "    class_label[~long_wins & (label['short_wealth'] > epsilon)] = -1\n",
    "    class_label = class_label.dropna().astype(int)\n",
    "    return pd.get_dummies(class_label, columns=['short', 'neutral', 'long']).astype(int)\n",
    "\n",
    "make_class(night_train_chunk.label, 1.0001).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "epsilon=1.0001\n",
    "\n",
    "for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
    "    X = chunk.lob.loc[:, hp.features].join(chunk.factor)\n",
    "    y = make_class(chunk.label, epsilon)\n",
    "    X = X.reindex(y.index)\n",
    "    y = y.reindex(X.index)\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat(X_list));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228842\n",
      "132460\n",
      "519114\n",
      "880416\n"
     ]
    }
   ],
   "source": [
    "print(morning_train_chunk.label.dropna().shape[0])\n",
    "print(afternoon_train_chunk.label.dropna().shape[0])\n",
    "print(night_train_chunk.label.dropna().shape[0])\n",
    "print(morning_train_chunk.label.dropna().shape[0] + afternoon_train_chunk.label.dropna().shape[0] + night_train_chunk.label.dropna().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_list_list = []\n",
    "for i, chunk in enumerate([morning_train_chunk, afternoon_train_chunk, night_train_chunk]):\n",
    "\n",
    "    X = chunk.lob.loc[:, hp.features].join(chunk.factor)\n",
    "    X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
    "    y = make_class(chunk.label, epsilon)\n",
    "    X = X.reindex(y.index)\n",
    "    y = y.reindex(X.index)\n",
    "    #transform y here\n",
    "\n",
    "    sp_list : List[stride_data.SequencePair] = stride_data.create_train_val_sequence_cv(X, y, cv=hp.cv, lookback=hp.lookback, \n",
    "                                            batch_size=hp.batch_size, batch_no=hp.batch_no[i], \n",
    "                                            shuffle=hp.shuffle, trim=hp.trim, replace=hp.replace)\n",
    "\n",
    "    sp_list_list.append(sp_list)\n",
    "\n",
    "sequence_pair_list_per_cv = list(zip(*sp_list_list))\n",
    "sequence_list_per_cv = [stride_data.CombinedSequence(*[sequence_pair_list_per_cv[i][j].train_sequence \n",
    "                        for j in range(len(sequence_pair_list_per_cv[i]))])\n",
    "                        for i in range(len(sequence_pair_list_per_cv))]\n",
    "\n",
    "cv_test_tuple_dict = {}\n",
    "for cv, sp_list in enumerate(sequence_pair_list_per_cv):\n",
    "    X_list, y_list = list(zip(*[sp.test_tuple for sp in sp_list]))\n",
    "    test_tuple = np.vstack(X_list), np.vstack(y_list)\n",
    "    cv_test_tuple_dict[cv] = test_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_sequence_list = []\n",
    "for i, chunk in enumerate([morning_wf_chunk, afternoon_wf_chunk, night_wf_chunk]):\n",
    "\n",
    "    X = chunk.lob.loc[:, hp.features].join(chunk.factor)\n",
    "    X = pd.DataFrame(scaler.transform(X), index=X.index, columns=X.columns)\n",
    "    y = make_class(chunk.label, epsilon)\n",
    "    X = X.reindex(y.index)\n",
    "    y = y.reindex(X.index)\n",
    "    #transform y here\n",
    "\n",
    "    wf_sequence : stride_data.StrideData = stride_data.StrideData(X, y, lookback=hp.lookback, \n",
    "                                            batch_size=y.shape[0] - hp.lookback, batch_no=None, \n",
    "                                            shuffle=False, replace=False)\n",
    "\n",
    "    wf_sequence_list.append(wf_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running cv0\n",
      "Epoch 1/100\n",
      "880/880 [==============================] - 125s 134ms/step - loss: 0.0958 - val_loss: 0.0956\n",
      "Epoch 2/100\n",
      "880/880 [==============================] - 108s 123ms/step - loss: 0.0850 - val_loss: 0.0945\n",
      "Epoch 3/100\n",
      " 74/880 [=>............................] - ETA: 1:03 - loss: 0.0852"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Miti\\VSCodeProject\\smu_course\\qf624_group_project\\keras_model.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39madd(layers\u001b[39m.\u001b[39mDense(\u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m# add output node\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mloss_dict[hp\u001b[39m.\u001b[39mloss], optimizer\u001b[39m=\u001b[39mopt, metrics\u001b[39m=\u001b[39m[], weighted_metrics\u001b[39m=\u001b[39m[])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49msq,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m           use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m           validation_data\u001b[39m=\u001b[39;49mtest_tuple,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49mhp\u001b[39m.\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m           verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[csv_logger, early_stopper])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Miti/VSCodeProject/smu_course/qf624_group_project/keras_model.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Miti\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Miti\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:1790\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1783\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1784\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1787\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1788\u001b[0m ):\n\u001b[0;32m   1789\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1790\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1791\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1792\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:820\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 820\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    822\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    823\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:848\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    845\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    846\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    847\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    851\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    852\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:132\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    129\u001b[0m   (concrete_function, filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(\n\u001b[0;32m    130\u001b[0m       args, kwargs\n\u001b[0;32m    131\u001b[0m   )\n\u001b[1;32m--> 132\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    133\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    134\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1368\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1364\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1365\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1366\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1367\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1369\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1370\u001b[0m     args,\n\u001b[0;32m   1371\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1372\u001b[0m     executing_eagerly)\n\u001b[0;32m   1373\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:222\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    221\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 222\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    223\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    224\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    225\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    226\u001b[0m     )\n\u001b[0;32m    227\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    229\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    230\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    231\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    232\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "for cv_count, sq in enumerate(sequence_list_per_cv):\n",
    "    print('running cv' + str(cv_count))\n",
    "    csv_logger = CSVLogger(fr'{base_output_folder}\\callback_logs\\{run_prefix}\\_temp\\{run_prefix}_{cv_count}.csv')\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hp.init_learning_rate,\n",
    "        decay_steps=hp.decay_steps,\n",
    "        decay_rate=hp.lr_decay)\n",
    "    try:\n",
    "        opt = opt_dict[hp.opt](learning_rate=lr_schedule)\n",
    "    except Exception:\n",
    "        opt = opt_dict[hp.opt](learning_rate=hp.init_learning_rate)\n",
    "\n",
    "    test_tuple = cv_test_tuple_dict[cv_count]\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    for l, p in schematic_dict[hp.schematic]:\n",
    "        model.add(l(**p))\n",
    "        \n",
    "    model.add(layers.Dense(3, activation='softmax')) # add output node\n",
    "\n",
    "    model.compile(loss=loss_dict[hp.loss], optimizer=opt, metrics=[], weighted_metrics=[])\n",
    "    model.fit(x=sq,\n",
    "              use_multiprocessing=False,\n",
    "              validation_data=test_tuple,\n",
    "              epochs=hp.epochs,\n",
    "              verbose=1,\n",
    "              callbacks=[csv_logger, early_stopper])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 18s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(wf_sequence_list[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(wf_sequence_list[0][0][1], columns=['true_short','true_neutral','true_long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>neutral</th>\n",
       "      <th>long</th>\n",
       "      <th>true_short</th>\n",
       "      <th>true_neutral</th>\n",
       "      <th>true_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34990</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34991</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34992</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34993</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34994</th>\n",
       "      <td>0.212784</td>\n",
       "      <td>0.53729</td>\n",
       "      <td>0.249926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34995 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          short  neutral      long  true_short  true_neutral  true_long\n",
       "0      0.212784  0.53729  0.249926           0             1          0\n",
       "1      0.212784  0.53729  0.249925           0             1          0\n",
       "2      0.212784  0.53729  0.249925           0             1          0\n",
       "3      0.212784  0.53729  0.249926           0             1          0\n",
       "4      0.212784  0.53729  0.249926           0             1          0\n",
       "...         ...      ...       ...         ...           ...        ...\n",
       "34990  0.212784  0.53729  0.249926           0             1          0\n",
       "34991  0.212784  0.53729  0.249925           0             1          0\n",
       "34992  0.212784  0.53729  0.249926           0             1          0\n",
       "34993  0.212784  0.53729  0.249926           0             1          0\n",
       "34994  0.212784  0.53729  0.249926           0             1          0\n",
       "\n",
       "[34995 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = pd.DataFrame(output, columns=['short','neutral','long']).join(true_df)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true_short      0.126618\n",
       "true_neutral    0.688413\n",
       "true_long       0.184969\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[predicted['neutral'] == predicted.iloc[:, :3].max(1)].iloc[:,3:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_53 (LSTM)              (None, 32)                7040      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,267\n",
      "Trainable params: 7,203\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
